{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, nltk\n",
    "import regex as re\n",
    "import pandas as pd \n",
    "import langdetect as ld #plan to use this library to detect and strip instances in which language != English\n",
    "import textblob as tb #plan to use this library to detect and strip instances in which language != English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#L-8 Data Cleaner, Making a new corpus from only the Spanish-speaking English Language Learner Data(Data already segmented)\n",
    "#def l8_en_sp_dataclean():\n",
    "    #lc = 'lang-8-20111007-2.0/lang-8-20111007-L1-v2.dat'\n",
    "    #with open(lc) as file:\n",
    "        #with open(\"l8_ell_spanish.txt\",\"w\") as output:\n",
    "            #for line in file:\n",
    "                #data = json.loads(line, strict=False)\n",
    "                #if data[2]=='English' and data[3]=='Spanish':\n",
    "                    #output.write(line)\n",
    "                    \n",
    "#L-8 Alinger, Aligning Learner sentences with corrections/feedback\n",
    "def l8_en_spanish_aligner():\n",
    "    en_sp_corpus = 'l8_ell_spanish.txt'\n",
    "    with open(en_sp_corpus) as file:\n",
    "        with open('l8_ell_spanish_feedback.txt', 'w') as output:\n",
    "            for line in file:\n",
    "                data = json.loads(line, strict=False)\n",
    "                output.write(str([list(pair) for pair in zip(data[4],data[5])]) + '\\n')\n",
    "                \n",
    "#Calling functions\n",
    "#l8_en_sp_dataclean()                \n",
    "l8_en_spanish_aligner()\n",
    "                \n",
    "#Creating text files of both learner and correction sentences (Spanish natives learning English)\n",
    "learner_list = [] \n",
    "with open('l8_ell_spanish.txt') as file:\n",
    "    with open('l8_ell_spanish_learner_text.txt', 'w') as output:\n",
    "        for line in file:\n",
    "            data = json.loads(line, strict=False)\n",
    "            for l_t in data[4]:\n",
    "                learner_list.append(l_t)\n",
    "                output.write(str(l_t) + '\\n')\n",
    "                \n",
    "cor_list = []\n",
    "with open('l8_ell_spanish.txt') as file:\n",
    "    with open('l8_ell_spanish_correction_text.txt', 'w') as output:\n",
    "        for line in file:\n",
    "            data = json.loads(line, strict=False)\n",
    "            for i in data[5]:\n",
    "                cor_list.append(i)\n",
    "                output.write(str(i) + '\\n')\n",
    "                \n",
    "#Aligning Learner sentences with feedback (No correction sentences == Duplication of Learner Sentence)\n",
    "l_c_dict = {l:c for l,c in zip(learner_list, cor_list)}\n",
    "\n",
    "for k in l_c_dict:\n",
    "    if bool(l_c_dict[k]) == False:\n",
    "        l_c_dict[k] = k\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "#regex that captures text enclosed in []\n",
    "for k in l_c_dict:\n",
    "    a = re.search(r'\\[(.*)\\]', str(l_c_dict[k]))\n",
    "    if a !=None:\n",
    "        l_c_dict[k] = a.group(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking for number of Non-Corrections vs. Corrections\n",
    "#num = 0\n",
    "#for k,v in l_c_dict.items():\n",
    "    #if k==v:\n",
    "        #num+=0\n",
    "    #else:\n",
    "        #num+=1\n",
    "        \n",
    "#print(num)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Representing Learner / Corrector Sentences via Pandas\n",
    "#l_c_list = pd.DataFrame({'Learner_Input': learner_list, 'Corrections': cor_list})\n",
    "#l_c_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A few regex expressions meant to clean the corpus in terms of meta-data\n",
    "for v in l_c_dict:\n",
    "    a = re.sub(r'\\[([f\\-red]*)\\b[^\\]]*\\].*?\\/\\1\\]', '', str(l_c_dict[v])) #Captures text between [f-red] and[/f-red]\n",
    "    l_c_dict[v] = a\n",
    "    a = re.sub(r'\\[(\\/f-red)','', str(l_c_dict[v])) #[f-red\n",
    "    l_c_dict[v] = a\n",
    "    a = re.sub(r'\\[(f-red)\\]','', str(l_c_dict[v])) #[f-red]\n",
    "    l_c_dict[v] = a\n",
    "    a = re.sub(r'(\\/f-red)\\]','', str(l_c_dict[v])) #f-red]\n",
    "    l_c_dict[v] = a\n",
    "    a = re.sub(r'\\[([sline]*)\\b[^\\]]*\\].*?\\/\\1\\]', '', str(l_c_dict[v])) #Captures text between [sline] and [/sline]\n",
    "    l_c_dict[v] = a\n",
    "    a = re.sub(r'\\[(f-blue)\\]','', str(l_c_dict[v])) #Replaces [f-blue] for whitespace\n",
    "    l_c_dict[v] = a\n",
    "    a = re.sub(r'\\[(\\/f-blue)\\]','', str(l_c_dict[v])) #Replaces [/f-blue] for whitespace\n",
    "    l_c_dict[v] = a\n",
    "    a = re.sub(r'\\[(\\/f-blue)','', str(l_c_dict[v])) #subs [/f-blue for whitespace\n",
    "    l_c_dict[v] = a\n",
    "    a = re.sub(r'(\\/f-blue)\\]','', str(l_c_dict[v])) #subs /f-blue] for whitespace\n",
    "    l_c_dict[v] = a\n",
    "    a = re.sub(r'\\[(f-blue)','', str(l_c_dict[v])) #subs [f-blue for whitespace\n",
    "    l_c_dict[v] = a\n",
    "    a = re.sub(r'(f-blue)\\]','', str(l_c_dict[v])) #subs f-blue] for whitespace\n",
    "    l_c_dict[v] = a\n",
    "    a = re.sub(r'\\[(f-bold)\\]','', str(l_c_dict[v])) #subs [f-bold] for whitespace\n",
    "    l_c_dict[v] = a\n",
    "    a = re.sub(r'\\[(\\/f-bold)\\]','', str(l_c_dict[v])) #subs [/f-bold] for whitespace\n",
    "    l_c_dict[v] = a\n",
    "    a = re.sub(r'\\[(f-bold)','', str(l_c_dict[v])) #subs [f-bold for whitespace\n",
    "    l_c_dict[v] = a\n",
    "    a = re.sub(r'(f-bold)\\]','', str(l_c_dict[v])) #subs f-bold] for whitespace\n",
    "    l_c_dict[v] = a\n",
    "    a = re.sub(r'\\[(\\/f-bold)','', str(l_c_dict[v])) #subs [/f-bold for whitespace\n",
    "    l_c_dict[v] = a\n",
    "    a = re.sub(r'(\\/f-bold)\\]','', str(l_c_dict[v])) #subs /f-bold] for whitespace\n",
    "    l_c_dict[v] = a"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

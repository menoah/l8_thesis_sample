{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, nltk\n",
    "import regex as re\n",
    "import pandas as pd\n",
    "import spacy\n",
    "#from spacy import displacy\n",
    "from pathlib import Path\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "import numpy as np\n",
    "import csv\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Saving cleaned learner/corrector pairs into different output files (for ERRANT)\n",
    "with open('082019_lang8_sp_eng_learner_cor_sents.txt') as file:\n",
    "    with open('20191010_clean_l8_learner_sents.txt', 'w') as output:\n",
    "        with open('20191010_clean_l8_cor_sents.txt', 'w') as output_2:\n",
    "            for line in file:\n",
    "                line = line.strip('\\n')\n",
    "                line = line.split('\\t')\n",
    "                #some of the pairs did not separate by '\\t'\n",
    "                #if len(line) == 1:\n",
    "                    #print(line)\n",
    "                output.write(str(line[0])+ '\\n')\n",
    "                output_2.write(str(line[1]+ '\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of corrections:  10819 41.65158806544755\n",
      "Number uncorrected:  15156 58.34841193455246\n"
     ]
    }
   ],
   "source": [
    "with open(\"20191010_clean_l8_learner_sents.txt\") as fd:\n",
    "    file = fd.read().splitlines()\n",
    "    \n",
    "with open(\"20191010_clean_l8_cor_sents.txt\") as fd_2:\n",
    "    file_2 = fd_2.read().splitlines()\n",
    "    \n",
    "with open(\"20191012_l8_sp_eng_m2\") as fd_3:\n",
    "    file_3 = fd_3.read().splitlines()\n",
    "    \n",
    "#Aligning Learner sentences with feedback (No correction sentence == Duplicating Learner Sentence)\n",
    "uncor_num = 0\n",
    "cor_num = 0\n",
    "l_c_dict = {l:c for l,c in zip(file, file_2)}\n",
    "for k in l_c_dict:\n",
    "    #Captures text enclosed in []\n",
    "    a = re.search(r'\\[(.*)\\]', str(l_c_dict[k]))\n",
    "    if a:\n",
    "        l_c_dict[k] = a.group(1)\n",
    "    if bool(l_c_dict[k]) == False:\n",
    "        l_c_dict[k] = k\n",
    "    else:\n",
    "        pass\n",
    "#Checking for number of Non-Corrections vs. Corrections\n",
    "for k,v in l_c_dict.items():\n",
    "    if k==v:\n",
    "        uncor_num+=1\n",
    "    else:\n",
    "        cor_num+=1\n",
    "print('Number of corrections: ', cor_num, cor_num/len(l_c_dict.keys())*100)\n",
    "print('Number uncorrected: ', uncor_num, uncor_num/len(l_c_dict.keys()) *100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using spaCy in order to make a list of items containing dependencies and Part of speech (POS)\n",
    "#information for each token in every learner/corrector pair\n",
    "dep_file = []\n",
    "dep_file_2 = []\n",
    "\n",
    "for line in file:\n",
    "    dep_sentence = nlp(line)\n",
    "    deps = []\n",
    "    for token in dep_sentence:\n",
    "        l_tok = token.text + '_' + token.pos_ + '_' + token.dep_\n",
    "        deps.append(l_tok)\n",
    "    dep_file.append(deps)\n",
    "    deps = []\n",
    "\n",
    "for line_2 in file_2:\n",
    "    dep_sentence_2 = nlp(line_2)\n",
    "    deps_2 = []\n",
    "    for token_2 in dep_sentence_2:\n",
    "        c_tok = token_2.text + '_' + token_2.pos_ + '_' + token_2.dep_\n",
    "        deps_2.append(c_tok)\n",
    "    dep_file_2.append(deps_2)\n",
    "    deps_2 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Handles cases in which extracted learner/corrector sentence was empty\n",
    "for item in range(0, len(file) -1):\n",
    "    if file[item] == ' ':\n",
    "        file.pop(item)\n",
    "        file_2.pop(item)\n",
    "        dep_file.pop(item)\n",
    "        dep_file_2.pop(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_list = []\n",
    "currlist = []\n",
    "#counter = 0\n",
    "\n",
    "for line in open('20191012_l8_sp_eng_m2'):\n",
    "    line = line.rstrip()\n",
    "    if line == '':\n",
    "        if currlist != []:\n",
    "            #print(currlist)\n",
    "            e_list.append(currlist)\n",
    "            #counter += 1\n",
    "        currlist = []\n",
    "        continue\n",
    "    currlist.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[\"S he's still waiting for her to come back\",\n",
       "  'A -1 -1|||noop|||-NONE-|||REQUIRED|||-NONE-|||0'],\n",
       " [\"S He wasn't able to remember their first kiss.\",\n",
       "  'A -1 -1|||noop|||-NONE-|||REQUIRED|||-NONE-|||0'],\n",
       " ['S She would have done.', 'A -1 -1|||noop|||-NONE-|||REQUIRED|||-NONE-|||0'],\n",
       " [\"S He couldn't remember the first time they made love.\",\n",
       "  'A -1 -1|||noop|||-NONE-|||REQUIRED|||-NONE-|||0'],\n",
       " ['S She would have done.', 'A -1 -1|||noop|||-NONE-|||REQUIRED|||-NONE-|||0'],\n",
       " [\"S He wasn't able to even remember her voice, her smile, her smell...\",\n",
       "  'A -1 -1|||noop|||-NONE-|||REQUIRED|||-NONE-|||0'],\n",
       " ['S She would have done.', 'A -1 -1|||noop|||-NONE-|||REQUIRED|||-NONE-|||0'],\n",
       " ['S The only thing he was able to remember after loosing her was her hands.',\n",
       "  'A -1 -1|||noop|||-NONE-|||REQUIRED|||-NONE-|||0'],\n",
       " ['S The', 'A -1 -1|||noop|||-NONE-|||REQUIRED|||-NONE-|||0'],\n",
       " ['S way she used to touch softly his face.',\n",
       "  'A -1 -1|||noop|||-NONE-|||REQUIRED|||-NONE-|||0']]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e_list[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_l_object = [list(pair) for pair in zip(file, dep_file, file_2,dep_file_2,e_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = m_l_object\n",
    "\n",
    "with open('20200129_l8_eng_sp_error_output.json', 'w') as outfile:\n",
    "    json.dump(data, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('20200129_l8_eng_sp_error_output.json') as json_file:\n",
    "    m_l_object = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A soft whimper breaks the silence.',\n",
       " ['A_DET_det',\n",
       "  'soft_ADJ_amod',\n",
       "  'whimper_NOUN_nsubj',\n",
       "  'breaks_VERB_ROOT',\n",
       "  'the_DET_det',\n",
       "  'silence_NOUN_dobj',\n",
       "  '._PUNCT_punct'],\n",
       " 'A soft whimper breaks the silence.',\n",
       " ['A_DET_det',\n",
       "  'soft_ADJ_amod',\n",
       "  'whimper_NOUN_nsubj',\n",
       "  'breaks_VERB_ROOT',\n",
       "  'the_DET_det',\n",
       "  'silence_NOUN_dobj',\n",
       "  '._PUNCT_punct'],\n",
       " ['S A soft whimper breaks the silence.',\n",
       "  'A -1 -1|||noop|||-NONE-|||REQUIRED|||-NONE-|||0']]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_l_object[50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>he's still waiting for her to come back</td>\n",
       "      <td>[he_PRON_nsubj, 's_AUX_aux, still_ADV_advmod, ...</td>\n",
       "      <td>he's still waiting for her to come back</td>\n",
       "      <td>[he_PRON_nsubj, 's_AUX_aux, still_ADV_advmod, ...</td>\n",
       "      <td>[S he's still waiting for her to come back, A ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>He wasn't able to remember their first kiss.</td>\n",
       "      <td>[He_PRON_nsubj, was_AUX_ROOT, n't_PART_neg, ab...</td>\n",
       "      <td>He wasn't able to remember their first kiss.</td>\n",
       "      <td>[He_PRON_nsubj, was_AUX_ROOT, n't_PART_neg, ab...</td>\n",
       "      <td>[S He wasn't able to remember their first kiss...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>She would have done.</td>\n",
       "      <td>[She_PRON_nsubj, would_VERB_aux, have_AUX_aux,...</td>\n",
       "      <td>She would have done.</td>\n",
       "      <td>[She_PRON_nsubj, would_VERB_aux, have_AUX_aux,...</td>\n",
       "      <td>[S She would have done., A -1 -1|||noop|||-NON...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>He couldn't remember the first time they made ...</td>\n",
       "      <td>[He_PRON_nsubj, could_VERB_aux, n't_PART_neg, ...</td>\n",
       "      <td>He couldn't remember the first time they made ...</td>\n",
       "      <td>[He_PRON_nsubj, could_VERB_aux, n't_PART_neg, ...</td>\n",
       "      <td>[S He couldn't remember the first time they ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>She would have done.</td>\n",
       "      <td>[She_PRON_nsubj, would_VERB_aux, have_AUX_aux,...</td>\n",
       "      <td>She would have done.</td>\n",
       "      <td>[She_PRON_nsubj, would_VERB_aux, have_AUX_aux,...</td>\n",
       "      <td>[S She would have done., A -1 -1|||noop|||-NON...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0  \\\n",
       "0            he's still waiting for her to come back   \n",
       "1       He wasn't able to remember their first kiss.   \n",
       "2                               She would have done.   \n",
       "3  He couldn't remember the first time they made ...   \n",
       "4                               She would have done.   \n",
       "\n",
       "                                                   1  \\\n",
       "0  [he_PRON_nsubj, 's_AUX_aux, still_ADV_advmod, ...   \n",
       "1  [He_PRON_nsubj, was_AUX_ROOT, n't_PART_neg, ab...   \n",
       "2  [She_PRON_nsubj, would_VERB_aux, have_AUX_aux,...   \n",
       "3  [He_PRON_nsubj, could_VERB_aux, n't_PART_neg, ...   \n",
       "4  [She_PRON_nsubj, would_VERB_aux, have_AUX_aux,...   \n",
       "\n",
       "                                                   2  \\\n",
       "0            he's still waiting for her to come back   \n",
       "1       He wasn't able to remember their first kiss.   \n",
       "2                               She would have done.   \n",
       "3  He couldn't remember the first time they made ...   \n",
       "4                               She would have done.   \n",
       "\n",
       "                                                   3  \\\n",
       "0  [he_PRON_nsubj, 's_AUX_aux, still_ADV_advmod, ...   \n",
       "1  [He_PRON_nsubj, was_AUX_ROOT, n't_PART_neg, ab...   \n",
       "2  [She_PRON_nsubj, would_VERB_aux, have_AUX_aux,...   \n",
       "3  [He_PRON_nsubj, could_VERB_aux, n't_PART_neg, ...   \n",
       "4  [She_PRON_nsubj, would_VERB_aux, have_AUX_aux,...   \n",
       "\n",
       "                                                   4  \n",
       "0  [S he's still waiting for her to come back, A ...  \n",
       "1  [S He wasn't able to remember their first kiss...  \n",
       "2  [S She would have done., A -1 -1|||noop|||-NON...  \n",
       "3  [S He couldn't remember the first time they ma...  \n",
       "4  [S She would have done., A -1 -1|||noop|||-NON...  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pn_l8 = pd.DataFrame(m_l_object)\n",
    "pn_l8.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Learner Sentences:  25974\n",
      "Toal number of sentences with annotations:  10850\n",
      "Total number of annotations:  12883\n",
      "% of sentences annotated:  41.77254177254177\n",
      "Average number of annotations per sentence:  1.187373271889401\n"
     ]
    }
   ],
   "source": [
    "#How many learner sentences have annotations?\n",
    "\n",
    "#initializing a dictionary of total number of learner sentences\n",
    "l_sents = defaultdict(int)\n",
    "#initializing a dictionary of annotated sentences\n",
    "annot_sents = defaultdict(int)\n",
    "#total_annotations\n",
    "total_annots = 0\n",
    "\n",
    "for i in range(0, len(m_l_object)):\n",
    "    s = m_l_object[i][0]\n",
    "    l_sents[s]+=1\n",
    "    if i == 6623 or i == 9415 or i == 24820:\n",
    "        continue\n",
    "    no_op_search = re.search(r'\\|\\|\\|(noop)\\|\\|\\|', m_l_object[i][4][1])\n",
    "    if not no_op_search:\n",
    "        annot_sents[s]+=1\n",
    "        total_annots += 1\n",
    "    \n",
    "print(\"Unique Learner Sentences: \", len(l_sents.keys()))\n",
    "print(\"Toal number of sentences with annotations: \", len(annot_sents.keys()))\n",
    "print(\"Total number of annotations: \", total_annots)\n",
    "print(\"% of sentences annotated: \", len(annot_sents.keys())/len(l_sents.keys())*100)\n",
    "print(\"Average number of annotations per sentence: \", total_annots/len(annot_sents.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M:PRON she 15\n",
      "M:PRON i 88\n",
      "M:PRON it 293\n",
      "M:PRON they 27\n",
      "M:PRON he 16\n",
      "M:PRON we 11\n",
      "M:PRON me 18\n",
      "M:PRON him 2\n",
      "M:PRON them 18\n",
      "M:PRON us 3\n",
      "M:PRON total:  491\n",
      "R:PRON it 109\n",
      "R:PRON i 115\n",
      "R:PRON we 10\n",
      "R:PRON me 13\n",
      "R:PRON they 46\n",
      "R:PRON he 12\n",
      "R:PRON her 12\n",
      "R:PRON them 29\n",
      "R:PRON she 10\n",
      "R:PRON us 2\n",
      "R:PRON total:  358\n",
      "U:PRON it 1\n",
      "U:PRON i 1\n",
      "U:PRON total:  2\n"
     ]
    }
   ],
   "source": [
    "#Target pronoun list that will be stored and tracked for in a dictionary\n",
    "pronouns = '(I|me|we|us|he|him|she|her|it|they|them)\n",
    "pro_types = '(M:PRON|R:PRON|U:PRON)'\n",
    "pronoun_type_counter = defaultdict(lambda: defaultdict(int))\n",
    "pronoun_counter = defaultdict(int)\n",
    "\n",
    "for item in m_l_object:\n",
    "    #learner sentence\n",
    "    l_t = str(item[0])\n",
    "    #learner sentence dependencies\n",
    "    l_d = item[1]\n",
    "    #corrector sentence\n",
    "    c_t = str(item[2])\n",
    "    #corrector sentence dependencies\n",
    "    c_d = item[3]\n",
    "    \n",
    "    for e in item[4]:\n",
    "        pro_target = re.search(r'\\|\\|\\|' + pro_types + r'\\|\\|\\|' \n",
    "                               + r'.*\\b(?i)' + pronouns + r'\\b.*\\|\\|\\|', e)\n",
    "        if pro_target:\n",
    "            anno_type = pro_target.group(1)\n",
    "            current_pronoun = pro_target.group(2).lower()\n",
    "            pronoun_type_counter[anno_type][current_pronoun] += 1\n",
    "            #if anno_type == 'U:PRON' and current_pronoun == 'it':\n",
    "                #print(e)\n",
    "          \n",
    "#for p in pronoun_counter.keys():\n",
    "    #print(p, pronoun_counter[p])\n",
    "for a in pronoun_type_counter.keys():\n",
    "    total = 0\n",
    "    for p in pronoun_type_counter[a].keys():\n",
    "        total += pronoun_type_counter[a][p]\n",
    "        print(a,p,pronoun_type_counter[a][p])\n",
    "    print(a, 'total: ', total)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

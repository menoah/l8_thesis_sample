{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, nltk\n",
    "import regex as re\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from pathlib import Path\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "import numpy as np\n",
    "import csv\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Saving cleaned learner/corrector pairs into different output files\n",
    "with open('082019_lang8_sp_eng_learner_cor_sents.txt') as file:\n",
    "    with open('20191010_clean_l8_learner_sents.txt', 'w') as output:\n",
    "        with open('20191010_clean_l8_cor_sents.txt', 'w') as output_2:\n",
    "            for line in file:\n",
    "                line = line.strip('\\n')\n",
    "                line = line.split('\\t')\n",
    "                #if len(line) == 1:\n",
    "                    #print(line)\n",
    "                output.write(str(line[0])+ '\\n')\n",
    "                output_2.write(str(line[1]+ '\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of corrections:  10819 41.65158806544755\n",
      "Number uncorrected:  15156 58.34841193455246\n"
     ]
    }
   ],
   "source": [
    "with open(\"20191010_clean_l8_learner_sents.txt\") as fd:\n",
    "    file = fd.read().splitlines()\n",
    "    \n",
    "with open(\"20191010_clean_l8_cor_sents.txt\") as fd_2:\n",
    "    file_2 = fd_2.read().splitlines()\n",
    "    \n",
    "with open(\"20191012_l8_sp_eng_m2\") as fd_3:\n",
    "    file_3 = fd_3.read().splitlines()\n",
    "    \n",
    "#Aligning Learner sentences with feedback (No correction sentence == Duplicating Learner Sentence)\n",
    "uncor_num = 0\n",
    "cor_num = 0\n",
    "l_c_dict = {l:c for l,c in zip(file, file_2)}\n",
    "for k in l_c_dict:\n",
    "    #Captures text enclosed in []\n",
    "    a = re.search(r'\\[(.*)\\]', str(l_c_dict[k]))\n",
    "    if a:\n",
    "        l_c_dict[k] = a.group(1)\n",
    "    if bool(l_c_dict[k]) == False:\n",
    "        l_c_dict[k] = k\n",
    "    else:\n",
    "        pass\n",
    "#Checking for number of Non-Corrections vs. Corrections\n",
    "for k,v in l_c_dict.items():\n",
    "    if k==v:\n",
    "        uncor_num+=1\n",
    "    else:\n",
    "        cor_num+=1\n",
    "print('Number of corrections: ', cor_num, cor_num/len(l_c_dict.keys())*100)\n",
    "print('Number uncorrected: ', uncor_num, uncor_num/len(l_c_dict.keys()) *100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using spaCy in order to make a list of items containing dependencies and Part of speech (POS)\n",
    "#information for each token in every learner/corrector pair\n",
    "dep_file = []\n",
    "dep_file_2 = []\n",
    "\n",
    "for line in file:\n",
    "    dep_sentence = nlp(line)\n",
    "    deps = []\n",
    "    for token in dep_sentence:\n",
    "        l_tok = token.text + '_' + token.pos_ + '_' + token.dep_\n",
    "        deps.append(l_tok)\n",
    "    dep_file.append(deps)\n",
    "    deps = []\n",
    "\n",
    "for line_2 in file_2:\n",
    "    dep_sentence_2 = nlp(line_2)\n",
    "    deps_2 = []\n",
    "    for token_2 in dep_sentence_2:\n",
    "        c_tok = token_2.text + '_' + token_2.pos_ + '_' + token_2.dep_\n",
    "        deps_2.append(c_tok)\n",
    "    dep_file_2.append(deps_2)\n",
    "    deps_2 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Handles cases in which extracted learner/corrector sentence was empty\n",
    "for item in range(0, len(file) -1):\n",
    "    if file[item] == ' ':\n",
    "        file.pop(item)\n",
    "        file_2.pop(item)\n",
    "        dep_file.pop(item)\n",
    "        dep_file_2.pop(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_list = []\n",
    "currlist = []\n",
    "#counter = 0\n",
    "\n",
    "for line in open('20191012_l8_sp_eng_m2'):\n",
    "    line = line.rstrip()\n",
    "    if line == '':\n",
    "        if currlist != []:\n",
    "            #print(currlist)\n",
    "            e_list.append(currlist)\n",
    "            #counter += 1\n",
    "        currlist = []\n",
    "        continue\n",
    "    currlist.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[\"S he's still waiting for her to come back\",\n",
       "  'A -1 -1|||noop|||-NONE-|||REQUIRED|||-NONE-|||0'],\n",
       " [\"S He wasn't able to remember their first kiss.\",\n",
       "  'A -1 -1|||noop|||-NONE-|||REQUIRED|||-NONE-|||0'],\n",
       " ['S She would have done.', 'A -1 -1|||noop|||-NONE-|||REQUIRED|||-NONE-|||0'],\n",
       " [\"S He couldn't remember the first time they made love.\",\n",
       "  'A -1 -1|||noop|||-NONE-|||REQUIRED|||-NONE-|||0'],\n",
       " ['S She would have done.', 'A -1 -1|||noop|||-NONE-|||REQUIRED|||-NONE-|||0'],\n",
       " [\"S He wasn't able to even remember her voice, her smile, her smell...\",\n",
       "  'A -1 -1|||noop|||-NONE-|||REQUIRED|||-NONE-|||0'],\n",
       " ['S She would have done.', 'A -1 -1|||noop|||-NONE-|||REQUIRED|||-NONE-|||0'],\n",
       " ['S The only thing he was able to remember after loosing her was her hands.',\n",
       "  'A -1 -1|||noop|||-NONE-|||REQUIRED|||-NONE-|||0'],\n",
       " ['S The', 'A -1 -1|||noop|||-NONE-|||REQUIRED|||-NONE-|||0'],\n",
       " ['S way she used to touch softly his face.',\n",
       "  'A -1 -1|||noop|||-NONE-|||REQUIRED|||-NONE-|||0']]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e_list[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_l_object = [list(pair) for pair in zip(file, dep_file, file_2,dep_file_2,e_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The silence is surrounding her and she can help trembling while thinks about him.',\n",
       " ['The_DET_det',\n",
       "  'silence_NOUN_nsubj',\n",
       "  'is_AUX_aux',\n",
       "  'surrounding_VERB_ROOT',\n",
       "  'her_PRON_dobj',\n",
       "  'and_CCONJ_cc',\n",
       "  'she_PRON_nsubj',\n",
       "  'can_VERB_aux',\n",
       "  'help_VERB_ROOT',\n",
       "  'trembling_VERB_xcomp',\n",
       "  'while_SCONJ_mark',\n",
       "  'thinks_VERB_advcl',\n",
       "  'about_ADP_prep',\n",
       "  'him_PRON_pobj',\n",
       "  '._PUNCT_punct'],\n",
       " \"The silence is surrounding her, and she can't help trembling while she thinks about him.\",\n",
       " ['The_DET_det',\n",
       "  'silence_NOUN_nsubj',\n",
       "  'is_AUX_aux',\n",
       "  'surrounding_VERB_ROOT',\n",
       "  'her_PRON_dobj',\n",
       "  ',_PUNCT_punct',\n",
       "  'and_CCONJ_cc',\n",
       "  'she_PRON_nsubj',\n",
       "  'ca_VERB_aux',\n",
       "  \"n't_PART_neg\",\n",
       "  'help_VERB_conj',\n",
       "  'trembling_VERB_xcomp',\n",
       "  'while_SCONJ_mark',\n",
       "  'she_PRON_nsubj',\n",
       "  'thinks_VERB_advcl',\n",
       "  'about_ADP_prep',\n",
       "  'him_PRON_pobj',\n",
       "  '._PUNCT_punct'],\n",
       " ['S The silence is surrounding her and she can help trembling while thinks about him.',\n",
       "  'A 4 5|||R:OTHER|||her,|||REQUIRED|||-NONE-|||0',\n",
       "  \"A 7 8|||R:VERB:TENSE|||can't|||REQUIRED|||-NONE-|||0\",\n",
       "  'A 11 11|||M:PRON|||she|||REQUIRED|||-NONE-|||0']]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_l_object[55]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = m_l_object\n",
    "\n",
    "with open('20200129_l8_eng_sp_error_output.json', 'w') as outfile:\n",
    "    json.dump(data, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('20200129_l8_eng_sp_error_output.json') as json_file:\n",
    "    m_l_object = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M:PRON she 15\n",
      "M:PRON i 88\n",
      "M:PRON it 293\n",
      "M:PRON they 27\n",
      "M:PRON he 16\n",
      "M:PRON we 11\n",
      "M:PRON me 18\n",
      "M:PRON him 2\n",
      "M:PRON them 18\n",
      "M:PRON us 3\n",
      "M:PRON total:  491\n",
      "R:PRON it 109\n",
      "R:PRON i 115\n",
      "R:PRON we 10\n",
      "R:PRON me 13\n",
      "R:PRON they 46\n",
      "R:PRON he 12\n",
      "R:PRON her 12\n",
      "R:PRON them 29\n",
      "R:PRON she 10\n",
      "R:PRON us 2\n",
      "R:PRON total:  358\n",
      "U:PRON it 1\n",
      "U:PRON i 1\n",
      "U:PRON total:  2\n"
     ]
    }
   ],
   "source": [
    "#Target pronoun list that will be stored and tracked for in a dictionary\n",
    "pronouns = '(I|me|we|us|he|him|she|her|it|they|them)'\n",
    "#preps = '(on|in|for|with)'\n",
    "pro_types = '(M:PRON|R:PRON|U:PRON)'\n",
    "#prep_types = '(M:PREP|R:PREP|U:PREP)'\n",
    "pronoun_type_counter = defaultdict(lambda: defaultdict(int))\n",
    "pronoun_counter = defaultdict(int)\n",
    "\n",
    "\n",
    "for item in m_l_object:\n",
    "    #learner sentence\n",
    "    l_t = str(item[0])\n",
    "    #continue if not right part of corpus (== will yield corrected, >0 uncorrected)\n",
    "    #if annot_sents[l_t] == 0:\n",
    "            #continue\n",
    "    #learner sentence dependencies\n",
    "    l_d = item[1]\n",
    "    #corrector sentence\n",
    "    c_t = str(item[2])\n",
    "    #corrector sentence dependencies\n",
    "    c_d = item[3]\n",
    "    \n",
    "    for e in item[4]:\n",
    "        pro_target = re.search(r'\\|\\|\\|' + pro_types + r'\\|\\|\\|' \n",
    "                               + r'.*\\b(?i)' + pronouns + r'\\b.*\\|\\|\\|', e)\n",
    "        #print(l_t)\n",
    "        if pro_target:\n",
    "            anno_type = pro_target.group(1)\n",
    "            #print(anno_type)\n",
    "            current_pronoun = pro_target.group(2).lower()\n",
    "            #print(l_t, current_pronoun, e)\n",
    "            #pronoun_type_counter[anno_type] += 1\n",
    "            pronoun_type_counter[anno_type][current_pronoun] += 1\n",
    "            #pronoun_counter[current_pronoun] += 1\n",
    "            #if anno_type == 'U:PRON' and current_pronoun == 'it':\n",
    "                #print(e)\n",
    "          \n",
    "#for p in pronoun_counter.keys():\n",
    "    #print(p, pronoun_counter[p])\n",
    "for a in pronoun_type_counter.keys():\n",
    "    total = 0\n",
    "    for p in pronoun_type_counter[a].keys():\n",
    "        total += pronoun_type_counter[a][p]\n",
    "        print(a,p,pronoun_type_counter[a][p])\n",
    "    print(a, 'total: ', total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pronoun_type_counter['M:PRON'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>he's still waiting for her to come back</td>\n",
       "      <td>[he_PRON_nsubj, 's_AUX_aux, still_ADV_advmod, ...</td>\n",
       "      <td>he's still waiting for her to come back</td>\n",
       "      <td>[he_PRON_nsubj, 's_AUX_aux, still_ADV_advmod, ...</td>\n",
       "      <td>[S he's still waiting for her to come back, A ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>He wasn't able to remember their first kiss.</td>\n",
       "      <td>[He_PRON_nsubj, was_AUX_ROOT, n't_PART_neg, ab...</td>\n",
       "      <td>He wasn't able to remember their first kiss.</td>\n",
       "      <td>[He_PRON_nsubj, was_AUX_ROOT, n't_PART_neg, ab...</td>\n",
       "      <td>[S He wasn't able to remember their first kiss...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>She would have done.</td>\n",
       "      <td>[She_PRON_nsubj, would_VERB_aux, have_AUX_aux,...</td>\n",
       "      <td>She would have done.</td>\n",
       "      <td>[She_PRON_nsubj, would_VERB_aux, have_AUX_aux,...</td>\n",
       "      <td>[S She would have done., A -1 -1|||noop|||-NON...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>He couldn't remember the first time they made ...</td>\n",
       "      <td>[He_PRON_nsubj, could_VERB_aux, n't_PART_neg, ...</td>\n",
       "      <td>He couldn't remember the first time they made ...</td>\n",
       "      <td>[He_PRON_nsubj, could_VERB_aux, n't_PART_neg, ...</td>\n",
       "      <td>[S He couldn't remember the first time they ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>She would have done.</td>\n",
       "      <td>[She_PRON_nsubj, would_VERB_aux, have_AUX_aux,...</td>\n",
       "      <td>She would have done.</td>\n",
       "      <td>[She_PRON_nsubj, would_VERB_aux, have_AUX_aux,...</td>\n",
       "      <td>[S She would have done., A -1 -1|||noop|||-NON...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0  \\\n",
       "0            he's still waiting for her to come back   \n",
       "1       He wasn't able to remember their first kiss.   \n",
       "2                               She would have done.   \n",
       "3  He couldn't remember the first time they made ...   \n",
       "4                               She would have done.   \n",
       "\n",
       "                                                   1  \\\n",
       "0  [he_PRON_nsubj, 's_AUX_aux, still_ADV_advmod, ...   \n",
       "1  [He_PRON_nsubj, was_AUX_ROOT, n't_PART_neg, ab...   \n",
       "2  [She_PRON_nsubj, would_VERB_aux, have_AUX_aux,...   \n",
       "3  [He_PRON_nsubj, could_VERB_aux, n't_PART_neg, ...   \n",
       "4  [She_PRON_nsubj, would_VERB_aux, have_AUX_aux,...   \n",
       "\n",
       "                                                   2  \\\n",
       "0            he's still waiting for her to come back   \n",
       "1       He wasn't able to remember their first kiss.   \n",
       "2                               She would have done.   \n",
       "3  He couldn't remember the first time they made ...   \n",
       "4                               She would have done.   \n",
       "\n",
       "                                                   3  \\\n",
       "0  [he_PRON_nsubj, 's_AUX_aux, still_ADV_advmod, ...   \n",
       "1  [He_PRON_nsubj, was_AUX_ROOT, n't_PART_neg, ab...   \n",
       "2  [She_PRON_nsubj, would_VERB_aux, have_AUX_aux,...   \n",
       "3  [He_PRON_nsubj, could_VERB_aux, n't_PART_neg, ...   \n",
       "4  [She_PRON_nsubj, would_VERB_aux, have_AUX_aux,...   \n",
       "\n",
       "                                                   4  \n",
       "0  [S he's still waiting for her to come back, A ...  \n",
       "1  [S He wasn't able to remember their first kiss...  \n",
       "2  [S She would have done., A -1 -1|||noop|||-NON...  \n",
       "3  [S He couldn't remember the first time they ma...  \n",
       "4  [S She would have done., A -1 -1|||noop|||-NON...  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pn_l8 = pd.DataFrame(m_l_object)\n",
    "pn_l8.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#how many learner sentences were corrected\n",
    "#total number of learner sentences\n",
    "l_sents = defaultdict(int)\n",
    "#annotated sentences\n",
    "annot_sents = defaultdict(int)\n",
    "#total_annotations\n",
    "total_annots = 0\n",
    "\n",
    "for i in range(0, len(m_l_object)):\n",
    "    s = m_l_object[i][0]\n",
    "    l_sents[s]+=1\n",
    "    if i == 6623 or i == 9415 or i == 24820:\n",
    "        continue\n",
    "    no_op_search = re.search(r'\\|\\|\\|(noop)\\|\\|\\|', m_l_object[i][4][1])\n",
    "    if not no_op_search:\n",
    "        annot_sents[s]+=1\n",
    "        total_annots += 1\n",
    "    \n",
    "#Unique Learner sentences\n",
    "print(len(l_sents.keys()))\n",
    "#Learner sentences with annotations\n",
    "print(len(annot_sents.keys()))\n",
    "#Total annotations\n",
    "print(total_annots)\n",
    "\n",
    "print(10850/25974 *100)\n",
    "\n",
    "print(12883/10850)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pronoun_counter = defaultdict(int)\n",
    "\n",
    "#target pronoun list\n",
    "pronouns = '(I|me|we|us|he|him|she|her|it|they|them)'\n",
    "\n",
    "pro_target = re.search(r'\\|\\|\\|(M:PRON)\\|\\|\\|.*' + '\\b(?i)' + pronouns + '\\b.*\\|\\|\\|', l_t)\n",
    "\n",
    "\n",
    "#Learner pronoun usage over all sentences\n",
    "pro_counter_l = 0\n",
    "pro_counter_l_i = 0\n",
    "pro_counter_l_me = 0\n",
    "pro_counter_l_you = 0\n",
    "pro_counter_l_it = 0\n",
    "pro_counter_l_it_nsubj = 0\n",
    "pro_counter_l_it_nsubjpass = 0\n",
    "pro_counter_l_it_dobj = 0\n",
    "pro_counter_l_he = 0\n",
    "pro_counter_l_she = 0\n",
    "pro_counter_l_him = 0\n",
    "pro_counter_l_her = 0\n",
    "pro_counter_l_we = 0\n",
    "pro_counter_l_they = 0\n",
    "pro_counter_l_them = 0\n",
    "pro_counter_l_us = 0\n",
    "\n",
    "#Pronoun counts in Corrector Sentences\n",
    "pro_counter_c_m = 0\n",
    "\n",
    "pro_counter_c = 0\n",
    "pro_counter_c_i = 0\n",
    "pro_counter_c_me = 0\n",
    "pro_counter_c_you = 0\n",
    "pro_counter_c_it = 0\n",
    "pro_counter_c_it_nsubj = 0\n",
    "pro_counter_c_it_nsubjpass = 0\n",
    "pro_counter_c_it_dobj = 0\n",
    "pro_counter_c_he = 0\n",
    "pro_counter_c_she = 0\n",
    "pro_counter_c_him = 0\n",
    "pro_counter_c_her = 0\n",
    "pro_counter_c_we = 0\n",
    "pro_counter_c_they = 0\n",
    "pro_counter_c_them = 0\n",
    "pro_counter_c_us = 0\n",
    "\n",
    "#stats on insertions\n",
    "#all pronoun insertions\n",
    "m_pron_counter = 0\n",
    "\n",
    "i_insert = 0\n",
    "i_insert_nsubj = 0\n",
    "\n",
    "me_insert = 0\n",
    "me_insert_nsubj = 0\n",
    "me_insert_dobj = 0\n",
    "\n",
    "you_insert = 0\n",
    "you_insert_nsubj = 0\n",
    "you_insert_dobj = 0\n",
    "\n",
    "it_insert = 0\n",
    "it_insert_nsubj = 0\n",
    "it_insert_dobj = 0\n",
    "\n",
    "me_insert = 0\n",
    "\n",
    "he_insert = 0\n",
    "he_insert_nsubj = 0\n",
    "he_insert_dobj = 0\n",
    "\n",
    "she_insert = 0\n",
    "she_insert_nsubj = 0\n",
    "she_insert_dobj = 0\n",
    "\n",
    "him_insert = 0\n",
    "him_insert_nsubj = 0\n",
    "him_insert_dobj = 0\n",
    "\n",
    "her_insert = 0\n",
    "her_insert_nsubj = 0\n",
    "her_insert_dobj = 0\n",
    "\n",
    "we_insert = 0\n",
    "we_insert_nsubj = 0\n",
    "we_insert_dobj = 0\n",
    "\n",
    "us_insert = 0\n",
    "us_insert_nsubj = 0\n",
    "us_insert_dobj = 0\n",
    "\n",
    "they_insert = 0\n",
    "they_insert_nsubj = 0\n",
    "they_insert_dobj = 0\n",
    "\n",
    "them_insert = 0\n",
    "them_insert_nsubj = 0\n",
    "them_insert_dobj = 0\n",
    "\n",
    "for item in m_l_object:\n",
    "    #learner sentence\n",
    "    l_t = str(item[0])\n",
    "    #continue if not right part of corpus (== will yield corrected, >0 uncorrected)\n",
    "    if annot_sents[l_t] == 0:\n",
    "            continue\n",
    "    #learner sentence dependencies\n",
    "    l_d = item[1]\n",
    "    #corrector sentence\n",
    "    c_t = str(item[2])\n",
    "    #corrector sentence dependencies\n",
    "    c_d = item[3]\n",
    "    \n",
    "    for e in item[4]:\n",
    "            it_search = re.search(r'^A (\\d+) (\\d+)\\|\\|\\|([^\\|]+)\\|\\|\\|([^\\|]*)\\|\\|\\|', e)\n",
    "            if it_search:\n",
    "                #beginning position of token offset\n",
    "                #b_pos = int(it_search.group(1))\n",
    "                #ending position of token offset\n",
    "                #e_pos = int(it_search.group(2))\n",
    "                #annotation type ERRANT produced(M,R,U)\n",
    "                ann_type = it_search.group(3)\n",
    "                #word(s) that underwent action\n",
    "                target = str(it_search.group(4))\n",
    "                if ann_type == 'M:PRON':\n",
    "                    #550 Insertions\n",
    "                    pro_counter_c_m +=1\n",
    "                    \n",
    "                    if target == 'I':\n",
    "                        i_insert += 1\n",
    "                        for c in c_d:\n",
    "                            pro_search = re.search(target + r'\\_(PRON)\\_' r'(.*)', c)\n",
    "                            if pro_search:\n",
    "                                #print(pro_search.group())\n",
    "                                dep_list = c.split('_')\n",
    "                                #print(dep_list)\n",
    "                        if dep_list[2] == 'nsubj':\n",
    "                            i_insert_nsubj += 1\n",
    "                            \n",
    "                    elif target == 'me':\n",
    "                        me_insert += 1\n",
    "                        for c in c_d:\n",
    "                            pro_search = re.search(target + r'\\_(PRON)\\_' r'(.*)', c)\n",
    "                            if pro_search:\n",
    "                                #print(pro_search.group())\n",
    "                                dep_list = c.split('_')\n",
    "                                #print(dep_list)\n",
    "                        if dep_list[2] == 'nsubj':\n",
    "                            me_insert_nsubj += 1\n",
    "                        elif dep_list[2] == 'dobj':\n",
    "                            me_insert_dobj +=1\n",
    "                            \n",
    "                    elif target == 'you':\n",
    "                        you_insert += 1\n",
    "                        for c in c_d:\n",
    "                            pro_search = re.search(target + r'\\_(PRON)\\_' r'(.*)', c)\n",
    "                            if pro_search:\n",
    "                                #print(pro_search.group())\n",
    "                                dep_list = c.split('_')\n",
    "                                #print(dep_list)\n",
    "                        if dep_list[2] == 'nsubj':\n",
    "                            you_insert_nsubj +=1\n",
    "                        elif dep_list[2] == 'dobj':\n",
    "                            you_insert_dobj +=1\n",
    "                            \n",
    "                    elif target == 'it':\n",
    "                        it_insert += 1\n",
    "                        for c in c_d:\n",
    "                            pro_search = re.search(target + r'\\_(PRON)\\_' r'(.*)', c)\n",
    "                            if pro_search:\n",
    "                                #print(pro_search.group())\n",
    "                                dep_list = c.split('_')\n",
    "                                #print(dep_list)\n",
    "                        if dep_list[2] == 'nsubj':\n",
    "                            it_insert_nsubj +=1\n",
    "                        elif dep_list[2] == 'dobj':\n",
    "                            it_insert_dobj +=1\n",
    "                                \n",
    "                    elif target == 'It':\n",
    "                        it_insert += 1\n",
    "                        for c in c_d:\n",
    "                            pro_search = re.search(target + r'\\_(PRON)\\_' r'(.*)', c)\n",
    "                            if pro_search:\n",
    "                                #print(pro_search.group())\n",
    "                                dep_list = c.split('_')\n",
    "                                #print(dep_list)\n",
    "                        if dep_list[2] == 'nsubj':\n",
    "                            it_insert_nsubj +=1\n",
    "                        elif dep_list[2] == 'dobj':\n",
    "                            it_insert_dobj +=1\n",
    "                            \n",
    "                    elif target == 'IT':\n",
    "                        it_insert += 1\n",
    "                        for c in c_d:\n",
    "                            pro_search = re.search(target + r'\\_(PRON)\\_' r'(.*)', c)\n",
    "                            if pro_search:\n",
    "                                #print(pro_search.group())\n",
    "                                dep_list = c.split('_')\n",
    "                                #print(dep_list)\n",
    "                        if dep_list[2] == 'nsubj':\n",
    "                            it_insert_nsubj +=1\n",
    "                        elif dep_list[2] == 'dobj':\n",
    "                            it_insert_dobj +=1\n",
    "                            \n",
    "                    elif target == 'he':\n",
    "                        he_insert += 1\n",
    "                        for c in c_d:\n",
    "                            pro_search = re.search(target + r'\\_(PRON)\\_' r'(.*)', c)\n",
    "                            if pro_search:\n",
    "                                #print(pro_search.group())\n",
    "                                dep_list = c.split('_')\n",
    "                                #print(dep_list)\n",
    "                        if dep_list[2] == 'nsubj':\n",
    "                            he_insert_nsubj +=1\n",
    "                        elif dep_list[2] == 'dobj':\n",
    "                            he_insert_dobj +=1\n",
    "                            \n",
    "                    elif target == 'she':\n",
    "                        she_insert += 1\n",
    "                        for c in c_d:\n",
    "                            pro_search = re.search(target + r'\\_(PRON)\\_' r'(.*)', c)\n",
    "                            if pro_search:\n",
    "                                #print(pro_search.group())\n",
    "                                dep_list = c.split('_')\n",
    "                                #print(dep_list)\n",
    "                        if dep_list[2] == 'nsubj':\n",
    "                            she_insert_nsubj +=1\n",
    "                        elif dep_list[2] == 'dobj':\n",
    "                            she_insert_dobj +=1\n",
    "                            \n",
    "                    elif target == 'him':\n",
    "                        him_insert += 1\n",
    "                        for c in c_d:\n",
    "                            pro_search = re.search(target + r'\\_(PRON)\\_' r'(.*)', c)\n",
    "                            if pro_search:\n",
    "                                #print(pro_search.group())\n",
    "                                dep_list = c.split('_')\n",
    "                                #print(dep_list)\n",
    "                        if dep_list[2] == 'nsubj':\n",
    "                            him_insert_nsubj +=1\n",
    "                        elif dep_list[2] == 'dobj':\n",
    "                            him_insert_dobj +=1\n",
    "                            \n",
    "                    elif target == 'her':\n",
    "                        her_insert += 1\n",
    "                        for c in c_d:\n",
    "                            pro_search = re.search(target + r'\\_(PRON)\\_' r'(.*)', c)\n",
    "                            if pro_search:\n",
    "                                #print(pro_search.group())\n",
    "                                dep_list = c.split('_')\n",
    "                                #print(dep_list)\n",
    "                        if dep_list[2] == 'nsubj':\n",
    "                            her_insert_nsubj +=1\n",
    "                        elif dep_list[2] == 'dobj':\n",
    "                            her_insert_dobj +=1\n",
    "                            \n",
    "                    elif target == 'we':\n",
    "                        we_insert += 1\n",
    "                        for c in c_d:\n",
    "                            pro_search = re.search(target + r'\\_(PRON)\\_' r'(.*)', c)\n",
    "                            if pro_search:\n",
    "                                #print(pro_search.group())\n",
    "                                dep_list = c.split('_')\n",
    "                                #print(dep_list)\n",
    "                        if dep_list[2] == 'nsubj':\n",
    "                            we_insert_nsubj +=1\n",
    "                        elif dep_list[2] == 'dobj':\n",
    "                            we_insert_dobj +=1\n",
    "                        \n",
    "                    elif target == 'us':\n",
    "                        us_insert += 1\n",
    "                        for c in c_d:\n",
    "                            pro_search = re.search(target + r'\\_(PRON)\\_' r'(.*)', c)\n",
    "                            if pro_search:\n",
    "                                #print(pro_search.group())\n",
    "                                dep_list = c.split('_')\n",
    "                                #print(dep_list)\n",
    "                        if dep_list[2] == 'nsubj':\n",
    "                            us_insert_nsubj +=1\n",
    "                        elif dep_list[2] == 'dobj':\n",
    "                            us_insert_dobj +=1\n",
    "                    \n",
    "                    elif target == 'they':\n",
    "                        they_insert += 1\n",
    "                        for c in c_d:\n",
    "                            pro_search = re.search(target + r'\\_(PRON)\\_' r'(.*)', c)\n",
    "                            if pro_search:\n",
    "                                #print(pro_search.group())\n",
    "                                dep_list = c.split('_')\n",
    "                                #print(dep_list)\n",
    "                        if dep_list[2] == 'nsubj':\n",
    "                            they_insert_nsubj +=1\n",
    "                        elif dep_list[2] == 'dobj':\n",
    "                            they_insert_dobj +=1\n",
    "                            \n",
    "                    elif target == 'them':\n",
    "                        them_insert += 1\n",
    "                        for c in c_d:\n",
    "                            pro_search = re.search(target + r'\\_(PRON)\\_' r'(.*)', c)\n",
    "                            if pro_search:\n",
    "                                #print(pro_search.group())\n",
    "                                dep_list = c.split('_')\n",
    "                                #print(dep_list)\n",
    "                        if dep_list[2] == 'nsubj':\n",
    "                            them_insert_nsubj +=1\n",
    "                        elif dep_list[2] == 'dobj':\n",
    "                            them_insert_dobj +=1\n",
    "            \n",
    "            \n",
    "    for l in l_d:\n",
    "        #catpuring all pronouns in learner sentences\n",
    "        l_search = re.findall(r'\\_(PRON)\\_', l)\n",
    "        if l_search:\n",
    "            l_count = len(l_search)\n",
    "            pro_counter_l += l_count\n",
    "        #capturing all pronoun usage 'I' in learner sentences\n",
    "        l_i_search = re.findall(r'^(?i)(I)\\_', l)\n",
    "        if len(l_i_search) != 0:\n",
    "            l_i_count = len(l_i_search)\n",
    "            pro_counter_l_i += l_i_count\n",
    "        #capturing all pronoun usage 'me' in learner sentences\n",
    "        l_me_search = re.findall(r'^(?i)(Me)\\_', l)\n",
    "        if len(l_me_search) != 0:\n",
    "            l_me_count = len(l_me_search)\n",
    "            pro_counter_l_me += l_me_count\n",
    "        #capturing all pronoun usage 'you' in learner sentences\n",
    "        l_you_search = re.findall(r'^(?i)(You)\\_', l)\n",
    "        if len(l_you_search) != 0:\n",
    "            l_you_count = len(l_you_search)\n",
    "            pro_counter_l_you += l_you_count\n",
    "        #capturing all pronoun usage 'It' in learner sentences\n",
    "        l_it_search = re.findall(r'^(?i)(It)\\_(PRON)', l)\n",
    "        if len(l_it_search) != 0:\n",
    "            l_it_count = len(l_it_search)\n",
    "            pro_counter_l_it += l_it_count\n",
    "        #capturing all pronoun usage 'It' in learner sentences in 'nsubj' position\n",
    "        l_it_nsubj_search = re.findall(r'^(?i)(It)\\_(PRON)\\_(nsubj)', l)\n",
    "        if l_it_nsubj_search:\n",
    "            l_it_nsubj_search_count = len(l_it_nsubj_search)\n",
    "            pro_counter_l_it_nsubj += l_it_nsubj_search_count\n",
    "        #capturing all pronoun usage 'It' in learner sentences in 'nsubjpass' position\n",
    "        l_it_nsubjpass_search = re.findall(r'^(?i)(It)\\_(PRON)\\_(nsubjpass)', l)\n",
    "        if l_it_nsubjpass_search:\n",
    "            l_it_nsubjpass_search_count = len(l_it_nsubjpass_search)\n",
    "            pro_counter_l_it_nsubjpass += l_it_nsubjpass_search_count\n",
    "        #capturing all pronoun usage 'It' in learner sentences in 'dobj' position\n",
    "        l_it_dobj_search = re.findall(r'^(?i)(It)\\_(PRON)\\_(dobj)', l)\n",
    "        if l_it_dobj_search:\n",
    "            l_it_dobj_search_count = len(l_it_dobj_search)\n",
    "            pro_counter_l_it_dobj += l_it_dobj_search_count\n",
    "        #capturing all pronoun usage 'he' in learner sentences\n",
    "        l_he_search = re.findall(r'^(?i)(He)\\_(PRON)\\_(.*)', l)\n",
    "        if l_he_search:\n",
    "            l_he_search_count = len(l_he_search)\n",
    "            pro_counter_l_he += l_he_search_count\n",
    "        #capturing all pronoun usage 'She' in learner sentences\n",
    "        l_she_search = re.findall(r'^(?i)(She)\\_(PRON)\\_(.*)', l)\n",
    "        if l_she_search:\n",
    "            l_she_search_count = len(l_she_search)\n",
    "            pro_counter_l_she += l_she_search_count\n",
    "        #capturing all pronoun usage 'him' in learner sentences\n",
    "        l_him_search = re.findall(r'^(?i)(Him)\\_(PRON)\\_(.*)', l)\n",
    "        if l_him_search:\n",
    "            l_him_search_count = len(l_him_search)\n",
    "            pro_counter_l_him += l_him_search_count\n",
    "        #capturing all pronoun usage 'her' in learner sentences\n",
    "        l_her_search = re.findall(r'^(?i)(Her)\\_(PRON)\\_(.*)', l)\n",
    "        if l_her_search:\n",
    "            l_her_search_count = len(l_her_search)\n",
    "            pro_counter_l_her += l_her_search_count\n",
    "        #capturing all pronoun usage 'we' in learner sentences\n",
    "        l_we_search = re.findall(r'^(?i)(We)\\_(PRON)\\_(.*)', l)\n",
    "        if l_we_search:\n",
    "            l_we_search_count = len(l_we_search)\n",
    "            pro_counter_l_we += l_we_search_count\n",
    "        #capturing all pronoun usage 'us' in learner sentences\n",
    "        l_us_search = re.findall(r'^(?i)(Us)\\_(PRON)\\_(.*)', l)\n",
    "        if l_us_search:\n",
    "            l_us_search_count = len(l_us_search)\n",
    "            pro_counter_l_us += l_us_search_count\n",
    "        #capturing all pronoun usage 'they' in learner sentences\n",
    "        l_they_search = re.findall(r'^(?i)(They)\\_(PRON)\\_(.*)', l)\n",
    "        if l_they_search:\n",
    "            l_they_search_count = len(l_they_search)\n",
    "            pro_counter_l_they += l_they_search_count\n",
    "        #capturing all pronoun usage 'them' in learner sentences\n",
    "        l_them_search = re.findall(r'^(?i)(Them)\\_(PRON)\\_(.*)', l)\n",
    "        if l_them_search:\n",
    "            l_them_search_count = len(l_them_search)\n",
    "            pro_counter_l_them += l_them_search_count\n",
    "            \n",
    "            \n",
    "    for c in c_d:\n",
    "        #capturing all pronouns in corrector sentences\n",
    "        c_search = re.findall(r'\\_(PRON)\\_', c)\n",
    "        if c_search:\n",
    "            c_count = len(c_search)\n",
    "            pro_counter_c += c_count\n",
    "        #capturing pronoun usage 'I' in corrector sentences\n",
    "        c_i_search = re.findall(r'^(?i)(I)\\_', c)\n",
    "        if len(c_i_search) != 0:\n",
    "            c_i_count = len(c_i_search)\n",
    "            pro_counter_c_i += c_i_count\n",
    "        #capturing all pronoun usage 'me' in corrector sentences\n",
    "        c_me_search = re.findall(r'^(?i)(Me)\\_', c)\n",
    "        if len(c_me_search) != 0:\n",
    "            c_me_count = len(c_me_search)\n",
    "            pro_counter_c_me += c_me_count\n",
    "        #capturing all pronoun usage 'you' in learner sentences\n",
    "        c_you_search = re.findall(r'^(?i)(You)\\_', c)\n",
    "        if len(c_you_search) != 0:\n",
    "            c_you_count = len(c_you_search)\n",
    "            pro_counter_c_you += c_you_count\n",
    "        #capturing pronoun usage 'It' in corrector sentences\n",
    "        c_it_search = re.findall(r'^(?i)(It)\\_', c)\n",
    "        if len(c_it_search) != 0:\n",
    "            c_it_count = len(c_it_search)\n",
    "            pro_counter_c_it += c_it_count\n",
    "         #capturing all pronoun usage 'It' in corrector sentences in 'nsubj' position\n",
    "        c_it_nsubj_search = re.findall(r'^(?i)(It)\\_(PRON)\\_(nsubj)', c)\n",
    "        if c_it_nsubj_search:\n",
    "            c_it_nsubj_search_count = len(c_it_nsubj_search)\n",
    "            pro_counter_c_it_nsubj += c_it_nsubj_search_count\n",
    "        #capturing all pronoun usage 'It' in corrector sentences in 'nsubjpass' position\n",
    "        c_it_nsubjpass_search = re.findall(r'^(?i)(It)\\_(PRON)\\_(nsubjpass)', c)\n",
    "        if c_it_nsubjpass_search:\n",
    "            c_it_nsubjpass_search_count = len(c_it_nsubjpass_search)\n",
    "            pro_counter_c_it_nsubjpass += c_it_nsubjpass_search_count\n",
    "        #capturing all pronoun usage 'It' in corrector sentences in 'dobj' position\n",
    "        c_it_dobj_search = re.findall(r'^(?i)(It)\\_(PRON)\\_(dobj)', c)\n",
    "        if c_it_dobj_search:\n",
    "            c_it_dobj_search_count = len(c_it_dobj_search)\n",
    "            pro_counter_c_it_dobj += c_it_dobj_search_count\n",
    "        #capturing all pronoun usage 'he' in corrector sentences\n",
    "        c_he_search = re.findall(r'^(?i)(He)\\_(PRON)\\_(.*)', c)\n",
    "        if c_he_search:\n",
    "            c_he_search_count = len(c_he_search)\n",
    "            pro_counter_c_he += c_he_search_count\n",
    "        #capturing all pronoun usage 'She' in corrector sentences\n",
    "        c_she_search = re.findall(r'^(?i)(She)\\_(PRON)\\_(.*)', c)\n",
    "        if c_she_search:\n",
    "            c_she_search_count = len(c_she_search)\n",
    "            pro_counter_c_she += c_she_search_count\n",
    "        #capturing all pronoun usage 'him' in corrector sentences\n",
    "        c_him_search = re.findall(r'^(?i)(Him)\\_(PRON)\\_(.*)', c)\n",
    "        if c_him_search:\n",
    "            c_him_search_count = len(c_him_search)\n",
    "            pro_counter_c_him += c_him_search_count\n",
    "        #capturing all pronoun usage 'her' in corrector sentences\n",
    "        c_her_search = re.findall(r'^(?i)(Her)\\_(PRON)\\_(.*)', c)\n",
    "        if c_her_search:\n",
    "            c_her_search_count = len(c_her_search)\n",
    "            pro_counter_c_her += c_her_search_count\n",
    "        #capturing all pronoun usage 'we' in corrector sentences\n",
    "        c_we_search = re.findall(r'^(?i)(We)\\_(PRON)\\_(.*)', c)\n",
    "        if c_we_search:\n",
    "            c_we_search_count = len(c_we_search)\n",
    "            pro_counter_c_we += c_we_search_count\n",
    "        #capturing all pronoun usage 'us' in corrector sentences\n",
    "        c_us_search = re.findall(r'^(?i)(Us)\\_(PRON)\\_(.*)', c)\n",
    "        if c_us_search:\n",
    "            c_us_search_count = len(c_us_search)\n",
    "            pro_counter_c_us += c_us_search_count\n",
    "        #capturing all pronoun usage 'they' in learner sentences\n",
    "        c_they_search = re.findall(r'^(?i)(They)\\_(PRON)\\_(.*)', c)\n",
    "        if c_they_search:\n",
    "            c_they_search_count = len(c_they_search)\n",
    "            pro_counter_c_they += c_they_search_count\n",
    "        #capturing all pronoun usage 'them' in learner sentences\n",
    "        c_them_search = re.findall(r'^(?i)(Them)\\_(PRON)\\_(.*)', c)\n",
    "        if c_them_search:\n",
    "            c_them_search_count = len(c_them_search)\n",
    "            pro_counter_c_them += c_them_search_count\n",
    "    \n",
    "\n",
    "#general stats\n",
    "#print(m_pron_counter)\n",
    "print(\"Instances of Pronoun usage in learner sentences: \", pro_counter_l)\n",
    "print(\"Instances of Pronoun usage in corrector sentences: \", pro_counter_c)\n",
    "print('Number of corrections in total: ', num, num/len(l_c_dict.keys())*100)\n",
    "print(\"Instances of Pronouns 'inserted' by corrector: \", pro_counter_c_m)\n",
    "print(\"Learner pronoun accuracy over corrections: \", pro_counter_l/pro_counter_l+pro_counter_c)\n",
    "print(\"Pronoun  usage 'I' in learner sentences: \", pro_counter_l_i)\n",
    "print(\"Pronoun usage 'I' in corrector sentences: \", pro_counter_c_i)\n",
    "print(\"Number of times 'I' was inserted by corrector: \", i_insert)\n",
    "print(\"Number of times 'I' was inserted at 'nsubj' position: \", i_insert_nsubj)\n",
    "print(\"Pronoun usage 'me' in learner sentences: \", pro_counter_l_me)\n",
    "print(\"Pronoun usage 'me' in corrector sentences: \", pro_counter_c_me)\n",
    "print(\"Number of times 'me' was inserted by corrector: \", me_insert)\n",
    "print(\"Number of times 'me' was inserted by corrector (nsubj): \", me_insert_nsubj)\n",
    "print(\"Number of times 'me' was inserted by corrector (dobj): \", me_insert_dobj)\n",
    "print(\"Pronoun usage 'you' in learner sentences: \", pro_counter_l_you)\n",
    "print(\"Pronoun usage 'you' in corrector sentences: \", pro_counter_c_you)\n",
    "print(\"Number of times 'you' was inserted by corrector: \", you_insert)\n",
    "print(\"Number of times 'you' was inserted by corrector (nsubj): \", you_insert_nsubj)\n",
    "print(\"Number of times 'you' was inserted by corrector (dobj): \", you_insert_dobj)\n",
    "print(\"Pronoun usage 'It' in learner sentences: \", pro_counter_l_it)\n",
    "print(\"Pronoun usage 'It' in learner sentences (nsubj position): \", pro_counter_l_it_nsubj)\n",
    "print(\"Pronoun usage 'It' in learner sentences (dobj position): \", pro_counter_l_it_dobj)\n",
    "print(\"Pronoun usage 'It' in corrector sentences: \", pro_counter_c_it)\n",
    "print(\"Pronoun usage 'It' in corrector sentences (nsubj position): \", pro_counter_c_it_nsubj)\n",
    "print(\"Pronoun usage 'It' in corrector sentences (dobj position): \", pro_counter_c_it_dobj)\n",
    "print(\"Number of times 'It' was inserted by corrector: \", it_insert)\n",
    "print(\"Number of times 'It' was inserted by corrector (nsubj): \", it_insert_nsubj)\n",
    "print(\"Number of times 'It' was inserted by corrector (dobj): \", it_insert_dobj)\n",
    "print(\"Pronoun usage 'he' in learner setence: \", pro_counter_l_he)\n",
    "print(\"Pronoun usage 'he' in corrector sentence: \", pro_counter_c_he)\n",
    "print(\"Number of times 'he' was inserted by corrector: \", he_insert)\n",
    "print(\"Number of times 'he' was inserted by corrector (nsubj): \", he_insert_nsubj)\n",
    "print(\"Number of times 'he' was inserted by corrector (dobj): \", he_insert_dobj)\n",
    "print(\"Pronoun usage 'she' in learner sentence: \", pro_counter_l_she)\n",
    "print(\"Pronoun usage 'she' in corrector sentence: \", pro_counter_c_she)\n",
    "print(\"Number of times 'she' was inserted by corrector : \", she_insert)\n",
    "print(\"Number of times 'she' was inserted by corrector (nsubj): \", she_insert_nsubj)\n",
    "print(\"Number of times 'she' was inserted by corrector (dobj): \", she_insert_dobj)\n",
    "print(\"Pronoun usage 'him' in learner sentence: \", pro_counter_l_him)\n",
    "print(\"Pronoun usage 'him' in corrector sentence: \", pro_counter_c_him)\n",
    "print(\"Number of times 'him' was inserted by corrector : \", him_insert)\n",
    "print(\"Number of times 'him' was inserted by corrector (nsubj) : \", him_insert_nsubj)\n",
    "print(\"Number of times 'him' was inserted by corrector (dobj) : \", him_insert_dobj)\n",
    "print(\"Pronoun usage 'her' in learner sentence: \", pro_counter_l_her)\n",
    "print(\"Pronoun usage 'her' in corrector sentence: \", pro_counter_c_her)\n",
    "print(\"Number of times 'her' was inserted by corrector: \", her_insert)\n",
    "print(\"Number of times 'her' was inserted by corrector (nsubj) : \", her_insert_nsubj)\n",
    "print(\"Number of times 'her' was inserted by corrector (dobj) : \", her_insert_dobj)\n",
    "print(\"Pronoun usage 'we' in learner sentence: \", pro_counter_l_we)\n",
    "print(\"Pronoun usage 'we' in corrector sentence: \", pro_counter_c_we)\n",
    "print(\"Number of times 'we' was inserted by corrector: \", we_insert)\n",
    "print(\"Pronoun usage 'us' in learner sentence: \", pro_counter_l_us)\n",
    "print(\"Pronoun usage 'us' in corrector sentence: \", pro_counter_c_us)\n",
    "print(\"Number of times 'us' was inserted by corrector (nsubj) : \", us_insert_nsubj)\n",
    "print(\"Number of times 'us' was inserted by corrector (dobj) : \", us_insert_dobj)\n",
    "print(\"Pronoun usage 'they' in learner sentence: \", pro_counter_l_they)\n",
    "print(\"Pronoun usage 'they' in corrector sentence: \", pro_counter_c_they)\n",
    "print(\"Number of times 'they' was inserted by corrector: \", they_insert)\n",
    "print(\"Number of times 'they' was inserted by corrector (nsubj): \", they_insert_nsubj)\n",
    "print(\"Number of times 'they' was inserted by corrector (dobj): \", they_insert_dobj)\n",
    "print(\"Pronoun usage 'them' in learner sentence: \", pro_counter_l_them)\n",
    "print(\"Pronoun usage 'them' in corrector sentence: \", pro_counter_c_them)\n",
    "print(\"Number of times 'them' was inserted by corrector: \", them_insert)\n",
    "print(\"Number of times 'them' was inserted by corrector (nsubj): \", them_insert_nsubj)\n",
    "print(\"Number of times 'them' was inserted by corrector (dobj): \", them_insert_dobj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in m_l_object:\n",
    "    #learner sentence\n",
    "    l_t = nlp(item[0])\n",
    "    l_t = [i.text for i in l_t]\n",
    "    #continue if not right part of corpus (== will yield corrected, >0 uncorrected)\n",
    "    #if annot_sents[l_t] == 0:\n",
    "            #continue\n",
    "    #learner sentence dependencies\n",
    "    l_d = item[1]\n",
    "    #corrector sentence\n",
    "    c_t = str(item[2])\n",
    "    #corrector sentence dependencies\n",
    "    c_d = item[3]\n",
    "    \n",
    "    for e in item[4]:\n",
    "            run_search = re.search(r'^A (\\d+) (\\d+)\\|\\|\\|([^\\|]+)\\|\\|\\|([^\\|]*)\\|\\|\\|', e)\n",
    "            if run_search:\n",
    "                #beginning position of token offset\n",
    "                b_pos = int(run_search.group(1))\n",
    "                #ending position of token offset\n",
    "                e_pos = int(run_search.group(2))\n",
    "                #annotation type ERRANT produced(M,R,U)\n",
    "                ann_type = run_search.group(3)\n",
    "                #word(s) that underwent action\n",
    "                target = str(run_search.group(4))\n",
    "                if target == '.':\n",
    "                    print(l_t[b_pos])\n",
    "                        #if item == ',':\n",
    "                            #print(item)\n",
    "                            #print(l_t,c_t,ann_type)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, nltk\n",
    "import regex as re\n",
    "import pandas as pd\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "import numpy as np\n",
    "\n",
    "#possible filter for non-english sentences\n",
    "#from langdetect import detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating text files of both learner and correction sentences (Spanish natives learning English)\n",
    "learner_list = [] \n",
    "with open('l8_ell_spanish.txt') as file:\n",
    "    with open('l8_ell_spanish_learner_text.txt', 'w') as output:\n",
    "        for line in file:\n",
    "            data = json.loads(line, strict=False)\n",
    "            for l_t in data[4]:\n",
    "                learner_list.append(l_t)\n",
    "                output.write(str(l_t) + '\\n')\n",
    "                \n",
    "cor_list = []\n",
    "with open('l8_ell_spanish.txt') as file:\n",
    "    with open('l8_ell_spanish_correction_text.txt', 'w') as output:\n",
    "        for line in file:\n",
    "            data = json.loads(line, strict=False)\n",
    "            for i in data[5]:\n",
    "                cor_list.append(i)\n",
    "                output.write(str(i) + '\\n')\n",
    "                \n",
    "#Aligning Learner sentences with feedback (No correction sentences == Duplicating Learner Sentence)\n",
    "l_c_dict = {l:c for l,c in zip(learner_list, cor_list)}\n",
    "\n",
    "for k in l_c_dict:\n",
    "    if bool(l_c_dict[k]) == False:\n",
    "        l_c_dict[k] = k\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Captures text enclosed in []\n",
    "for k in l_c_dict:\n",
    "    a = re.search(r'\\[(.*)\\]', str(l_c_dict[k]))\n",
    "    if a !=None:\n",
    "        l_c_dict[k] = a.group(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking for number of Non-Corrections vs. Corrections\n",
    "\n",
    "#num = 0\n",
    "#for k,v in l_c_dict.items():\n",
    "    #if k==v:\n",
    "        #num+=0\n",
    "    #else:\n",
    "        #num+=1\n",
    "        \n",
    "#print(num)   \n",
    "#print(num/len(l_c_dict.keys())*100)\n",
    "#len(l_c_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Representing Learner / Corrector Sentences via Pandas\n",
    "#l_c_list = pd.DataFrame({'Learner_Input': learner_list, 'Corrections': cor_list})\n",
    "#l_c_list\n",
    "\n",
    "#l_c_d = pd.DataFrame(list(l_c_dict.items()))\n",
    "#l_c_d.columns = ['Learner_Sentence', 'Correction']\n",
    "#l_c_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Saving cleaned learner/corrector pairs into different output files\n",
    "with open('082019_lang8_sp_eng_learner_cor_sents.txt') as file:\n",
    "    with open('20191010_clean_l8_learner_sents.txt', 'w') as output:\n",
    "        with open('20191010_clean_l8_cor_sents.txt', 'w') as output_2:\n",
    "            for line in file:\n",
    "                line = line.strip('\\n')\n",
    "                line = line.split('\\t')\n",
    "                #Checking to make sure learner/corrector sentence pairs are separted by tab.\n",
    "                #if len(line) == 1:\n",
    "                    #print(line)\n",
    "                output.write(str(line[0])+ '\\n')\n",
    "                output_2.write(str(line[1]+ '\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Capturing Pro-Dropping (M:PRON) in learner sentences\n",
    "\n",
    "#running the sentence through spacy, check dependencies to confirm it is an nsubj Pro drop.\n",
    "with open('errant/20191012_l8_sp_eng_m2') as file:\n",
    "    error_list = []\n",
    "    #pd_counter = 0\n",
    "    pd_list = []\n",
    "    #Iterating over the lines of the error report\n",
    "    #Each learner sentence and corresponding annotation(s) separated by '\\n'\n",
    "    for line in file:\n",
    "        #Adding instance of learner sentence/annotations to a list\n",
    "        error_list.append(line)\n",
    "        if line == '\\n':\n",
    "            #print(error_list)\n",
    "            for item in error_list:\n",
    "                #If M:PRON is found in error report of sentence, capture original sentence, \n",
    "                #Else ignore\n",
    "                try:\n",
    "                    #re.search function replacing list elements\n",
    "                    pro_search = re.search(r'\\|\\|\\|(M:PRON)\\|\\|\\|', str(item))\n",
    "                    if pro_search.group(1) == \"M:PRON\":\n",
    "                        #print(\"MATCH CASE: \", error_list)\n",
    "                        pro_sentence = str(error_list[0])\n",
    "                        #removing sentence (S) annotation\n",
    "                        pro_sentence = re.sub(r'^(S)','', str(pro_sentence))\n",
    "                        #removing whitespace at the beginning of sentence\n",
    "                        pro_sentence = re.sub(r'^\\s', '', str(pro_sentence))\n",
    "                        #removing newline\n",
    "                        pro_sentence = re.sub(r'$(\\n)','', str(pro_sentence))\n",
    "                        pro_sentence = nlp(pro_sentence)\n",
    "                        print(len(pro_sentence))\n",
    "                        print(pro_sentence)\n",
    "                        for token in pro_sentence:\n",
    "                            pd_dep = token.dep_\n",
    "                            if 'nsubj' in pd_dep:\n",
    "                                #print(pd_dep)\n",
    "                                break\n",
    "                            else:\n",
    "                                pd_list.append(pro_sentence)\n",
    "                                #print(pro_sentence)\n",
    "                                continue\n",
    "                                #print(token.text, token.pos_, token.dep_)\n",
    "                            #try:\n",
    "                               # pd_dep = re.search(r'\\s(nsubj)', str(item))\n",
    "                                #if pd_dep.search.group(1) == 'nsubj':\n",
    "                                    #pass\n",
    "                            #except AttributeError:\n",
    "                                #print(pro_sentence)\n",
    "                            #looking for absence of nsubj, nsubjpass\n",
    "                            #if 'nsubj' or 'nsubjpass' in str(pd_dep):\n",
    "                                #print(\"nope\")\n",
    "                                #pass\n",
    "                            #else:\n",
    "                                #print(\"Hi\")\n",
    "                                #add counter\n",
    "                                #print(pro_sentence)\n",
    "                        #print(pro_sentence)\n",
    "                        #except AttributeError:\n",
    "                                #pass\n",
    "                                #print(\"No matches -- moving on\")\n",
    "            error_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Capturing learner sentences with word order (R:WO) annotations\n",
    "with open('errant/20191012_l8_sp_eng_m2') as file:\n",
    "    error_list = []\n",
    "    wo_list = []\n",
    "    wo_cor_list = []\n",
    "    #Iterating over the lines of the error report\n",
    "    #Each learner sentence and corresponding annotation(s) separated by '\\n'\n",
    "    for line in file:\n",
    "        #Adding instance of learner sentence/annotations to a list\n",
    "        error_list.append(line)\n",
    "        if line == '\\n':\n",
    "            #print(error_list)\n",
    "            for item in error_list:\n",
    "                #If M:PRON is found in error report of sentence, capture original sentence, \n",
    "                #Else ignore\n",
    "                try:\n",
    "                    #re.search function replacing list elements\n",
    "                    wo_search = re.search(r'\\|\\|\\|(R:WO)\\|\\|\\|', str(item))\n",
    "                    if wo_search.group(1) == \"R:WO\":\n",
    "                        try:\n",
    "                            correction_search = re.search(r'\\|\\|\\|([R:WO]*)\\|\\|\\|(.*?)\\|\\|\\|(REQUIRED)',\n",
    "                                                         str(item))\n",
    "                            print(correction_search.group(2))\n",
    "                            #print(\"MATCH CASE: \", error_list)\n",
    "                            wo_sentence = str(error_list[0])\n",
    "                            wo_sentence = re.sub(r'^(S)','', str(wo_sentence))\n",
    "                            #removing whitespace at the beginning of sentence\n",
    "                            pro_sentence = re.sub(r'^\\s', '', str(pro_sentence))\n",
    "                            wo_sentence = re.sub(r'$(\\n)','', str(wo_sentence))\n",
    "                            wo_sentence = nlp(wo_sentence)\n",
    "                            print(wo_sentence)\n",
    "                            break\n",
    "                    \n",
    "                        except AttributeError:\n",
    "                            pass\n",
    "                except AttributeError:\n",
    "                    pass\n",
    "                    #print(\"No matches -- moving on\")\n",
    "            error_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#accessing both files containing learner and corrector sentences, respectively\n",
    "with open(\"20191010_clean_l8_learner_sents.txt\") as fd:\n",
    "    file = fd.read().splitlines()\n",
    "    \n",
    "with open(\"20191010_clean_l8_cor_sents.txt\") as fd_2:\n",
    "    file_2 = fd_2.read().splitlines()\n",
    "\n",
    "#Reading in Error report generated by ERRANT, creating a nested list storing error reports\n",
    "#by instance, sentence annotations are separated by '\\n'.\n",
    "\n",
    "e_list = []\n",
    "currlist = []\n",
    "#counter = 0\n",
    "\n",
    "for line in open('errant/20191012_l8_sp_eng_m2'):\n",
    "    line = line.rstrip()\n",
    "    if line == '':\n",
    "        if currlist != []:\n",
    "            #print(currlist)\n",
    "            e_list.append(currlist)\n",
    "            #counter += 1\n",
    "        currlist = []\n",
    "        continue\n",
    "    currlist.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generating dependencies for learner sentences through SpaCy, storing as a nested list\n",
    "dep_file = []\n",
    "\n",
    "for line in file:\n",
    "    dep_sentence = nlp(line)\n",
    "    deps = []\n",
    "    for token in dep_sentence:\n",
    "        l_dep = token.dep_\n",
    "        deps.append(l_dep)\n",
    "    dep_file.append(deps)\n",
    "    deps = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generating dependencies for corrector sentences through SpaCy, storing as a nested list\n",
    "dep_file_2 = []\n",
    "\n",
    "for line in file_2:\n",
    "    dep_sentence = nlp(line)\n",
    "    deps = []\n",
    "    for token in dep_sentence:\n",
    "        l_dep = token.dep_\n",
    "        deps.append(l_dep)\n",
    "    dep_file_2.append(deps)\n",
    "    deps = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Zipping items into a list containing the following:[Learner sentence, [learner_dependencies],\n",
    "#Corrector sentences, [corrector_dependencies], Error annotations for sentence via ERRANT]\n",
    "\n",
    "m_l_object = []\n",
    "m_l_object = [list(pair) for pair in zip(file, dep_file, file_2,dep_file_2,e_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"He wasn't able to remember their first kiss.\",\n",
       " ['nsubj',\n",
       "  'ROOT',\n",
       "  'neg',\n",
       "  'acomp',\n",
       "  'aux',\n",
       "  'xcomp',\n",
       "  'poss',\n",
       "  'amod',\n",
       "  'dobj',\n",
       "  'punct'],\n",
       " \"He wasn't able to remember their first kiss.\",\n",
       " ['nsubj',\n",
       "  'ROOT',\n",
       "  'neg',\n",
       "  'acomp',\n",
       "  'aux',\n",
       "  'xcomp',\n",
       "  'poss',\n",
       "  'amod',\n",
       "  'dobj',\n",
       "  'punct'],\n",
       " [\"S He wasn't able to remember their first kiss.\",\n",
       "  'A -1 -1|||noop|||-NONE-|||REQUIRED|||-NONE-|||0']]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_l_object[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Someday we meet again',\n",
       " ['advmod', 'nsubj', 'ROOT', 'advmod'],\n",
       " 'Someday we will meet again',\n",
       " ['advmod', 'nsubj', 'aux', 'ROOT', 'advmod'],\n",
       " ['S Someday we meet again',\n",
       "  'A 2 2|||M:VERB:TENSE|||will|||REQUIRED|||-NONE-|||0']]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_l_object[374]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

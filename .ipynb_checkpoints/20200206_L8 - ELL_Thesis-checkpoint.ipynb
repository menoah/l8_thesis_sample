{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, nltk\n",
    "import regex as re\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from pathlib import Path\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "import numpy as np\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "\n",
    "#possible filter for non-english sentences\n",
    "#from langdetect import detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#L-8 Data Cleaner, Making a new corpus from only the Spanish-speaking English Language Learner Data\n",
    "def l8_en_sp_dataclean():\n",
    "    lc = 'lang-8-20111007-2.0/lang-8-20111007-L1-v2.dat'\n",
    "    with open(lc) as file:\n",
    "        with open(\"l8_ell_spanish.txt\",\"w\") as output:\n",
    "            for line in file:\n",
    "                data = json.loads(line, strict=False)\n",
    "                if data[2]=='English' and data[3]=='Spanish':\n",
    "                    output.write(line)\n",
    "                    \n",
    "#L-8 Alinger, Aligning Learner sentences with corrections/feedback\n",
    "def l8_en_spanish_aligner():\n",
    "    en_sp_corpus = 'l8_ell_spanish.txt'\n",
    "    with open(en_sp_corpus) as file:\n",
    "        with open('l8_ell_spanish_feedback.txt', 'w') as output:\n",
    "            for line in file:\n",
    "                data = json.loads(line, strict=False)\n",
    "                output.write(str([list(pair) for pair in zip(data[4],data[5])]) + '\\n')\n",
    "                \n",
    "#Calling functions\n",
    "l8_en_sp_dataclean()                \n",
    "l8_en_spanish_aligner()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating text files of both learner and correction sentences (Spanish natives learning English)\n",
    "learner_list = [] \n",
    "with open('l8_ell_spanish.txt') as file:\n",
    "    with open('l8_ell_spanish_learner_text.txt', 'w') as output:\n",
    "        for line in file:\n",
    "            data = json.loads(line, strict=False)\n",
    "            for l_t in data[4]:\n",
    "                learner_list.append(l_t)\n",
    "                output.write(str(l_t) + '\\n')\n",
    "                \n",
    "cor_list = []\n",
    "with open('l8_ell_spanish.txt') as file:\n",
    "    with open('l8_ell_spanish_correction_text.txt', 'w') as output:\n",
    "        for line in file:\n",
    "            data = json.loads(line, strict=False)\n",
    "            for i in data[5]:\n",
    "                cor_list.append(i)\n",
    "                output.write(str(i) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Saving cleaned learner/corrector pairs into different output files\n",
    "with open('082019_lang8_sp_eng_learner_cor_sents.txt') as file:\n",
    "    with open('20191010_clean_l8_learner_sents.txt', 'w') as output:\n",
    "        with open('20191010_clean_l8_cor_sents.txt', 'w') as output_2:\n",
    "            for line in file:\n",
    "                line = line.strip('\\n')\n",
    "                line = line.split('\\t')\n",
    "                #if len(line) == 1:\n",
    "                    #print(line)\n",
    "                output.write(str(line[0])+ '\\n')\n",
    "                output_2.write(str(line[1]+ '\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of corrections:  10820 41.65543792107796\n"
     ]
    }
   ],
   "source": [
    "#file = open('20191010_clean_l8_learner_sents.txt').readlines()\n",
    "#file_2 = open('20191010_clean_l8_cor_sents.txt').readlines()\n",
    "#file_3 = open('errant/20191012_l8_sp_eng_m2').readlines()\n",
    "#file_3 = #if this sentence == everything below until it hits the next sentence '\\n'\n",
    "\n",
    "with open(\"20191010_clean_l8_learner_sents.txt\") as fd:\n",
    "    file = fd.read().splitlines()\n",
    "    \n",
    "with open(\"20191010_clean_l8_cor_sents.txt\") as fd_2:\n",
    "    file_2 = fd_2.read().splitlines()\n",
    "    \n",
    "with open(\"20191012_l8_sp_eng_m2\") as fd_3:\n",
    "    file_3 = fd_3.read().splitlines()\n",
    "    \n",
    "#Aligning Learner sentences with feedback (No correction sentences == Duplicating Learner Sentence)\n",
    "num = 0\n",
    "l_c_dict = {l:c for l,c in zip(file, file_2)}\n",
    "for k in l_c_dict:\n",
    "    #Captures text enclosed in []\n",
    "    a = re.search(r'\\[(.*)\\]', str(l_c_dict[k]))\n",
    "    if a:\n",
    "        l_c_dict[k] = a.group(1)\n",
    "    if bool(l_c_dict[k]) == False:\n",
    "        l_c_dict[k] = k\n",
    "    else:\n",
    "        pass\n",
    "#Checking for number of Non-Corrections vs. Corrections\n",
    "for k,v in l_c_dict.items():\n",
    "    if k==v:\n",
    "        num+=0\n",
    "    else:\n",
    "        num+=1\n",
    "print('Number of corrections: ', num, num/len(l_c_dict.keys())*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_list = []\n",
    "c_list = []\n",
    "\n",
    "\n",
    "with open(\"20191010_clean_l8_learner_sents.txt\") as fd:\n",
    "    file = fd.read().splitlines()\n",
    "    \n",
    "with open(\"20191010_clean_l8_cor_sents.txt\") as fd_2:\n",
    "    file_2 = fd_2.read().splitlines()\n",
    "    \n",
    "    for line in range(0, len(file)):\n",
    "        temp_l_sent = []\n",
    "        temp_c_sent = []\n",
    "        l_sent = nlp(file[line])\n",
    "        c_sent = nlp(file_2[line])\n",
    "        for ch in l_sent:\n",
    "            temp_l_sent.append(ch.text)\n",
    "        l_list.append(temp_l_sent)\n",
    "        for ch_2 in c_sent:\n",
    "            temp_c_sent.append(ch_2.text)\n",
    "        c_list.append(temp_c_sent)\n",
    "        \n",
    "\n",
    "data = l_list\n",
    "data_2 = c_list\n",
    "        \n",
    "\n",
    "with open('20191206_l8_eng_sp_tok_ls.txt', 'w') as outfile:\n",
    "    with open('20191206_l8_eng_sp_tok_cs.txt', 'w') as outfile_2:\n",
    "        for i in data:\n",
    "            l_s = ' '.join(i)\n",
    "            outfile.write(l_s + '\\n')\n",
    "        for i_2 in data_2:\n",
    "            c_s = ' '.join(i_2)\n",
    "            outfile_2.write(c_s + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dep_file = []\n",
    "dep_file_2 = []\n",
    "\n",
    "for line in file:\n",
    "    dep_sentence = nlp(line)\n",
    "    deps = []\n",
    "    for token in dep_sentence:\n",
    "        l_tok = token.text + '_' + token.pos_ + '_' + token.dep_\n",
    "        deps.append(l_tok)\n",
    "    dep_file.append(deps)\n",
    "    deps = []\n",
    "\n",
    "for line_2 in file_2:\n",
    "    dep_sentence_2 = nlp(line_2)\n",
    "    deps_2 = []\n",
    "    for token_2 in dep_sentence_2:\n",
    "        c_tok = token_2.text + '_' + token_2.pos_ + '_' + token_2.dep_\n",
    "        deps_2.append(c_tok)\n",
    "    dep_file_2.append(deps_2)\n",
    "    deps_2 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in range(0, len(file)):\n",
    "    if file[item] == ' ':\n",
    "        file.pop(item)\n",
    "        file_2.pop(item)\n",
    "        dep_file.pop(item)\n",
    "        dep_file_2.pop(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_list = []\n",
    "currlist = []\n",
    "#counter = 0\n",
    "\n",
    "for line in open('20191012_l8_sp_eng_m2'):\n",
    "    line = line.rstrip()\n",
    "    if line == '':\n",
    "        if currlist != []:\n",
    "            #print(currlist)\n",
    "            e_list.append(currlist)\n",
    "            #counter += 1\n",
    "        currlist = []\n",
    "        continue\n",
    "    currlist.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_l_object = [list(pair) for pair in zip(file, dep_file, file_2,dep_file_2,e_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The silence is surrounding her and she can help trembling while thinks about him.',\n",
       " ['The_DET_det',\n",
       "  'silence_NOUN_nsubj',\n",
       "  'is_AUX_aux',\n",
       "  'surrounding_VERB_ROOT',\n",
       "  'her_PRON_dobj',\n",
       "  'and_CCONJ_cc',\n",
       "  'she_PRON_nsubj',\n",
       "  'can_VERB_aux',\n",
       "  'help_VERB_ROOT',\n",
       "  'trembling_VERB_xcomp',\n",
       "  'while_SCONJ_mark',\n",
       "  'thinks_VERB_advcl',\n",
       "  'about_ADP_prep',\n",
       "  'him_PRON_pobj',\n",
       "  '._PUNCT_punct'],\n",
       " \"The silence is surrounding her, and she can't help trembling while she thinks about him.\",\n",
       " ['The_DET_det',\n",
       "  'silence_NOUN_nsubj',\n",
       "  'is_AUX_aux',\n",
       "  'surrounding_VERB_ROOT',\n",
       "  'her_PRON_dobj',\n",
       "  ',_PUNCT_punct',\n",
       "  'and_CCONJ_cc',\n",
       "  'she_PRON_nsubj',\n",
       "  'ca_VERB_aux',\n",
       "  \"n't_PART_neg\",\n",
       "  'help_VERB_conj',\n",
       "  'trembling_VERB_xcomp',\n",
       "  'while_SCONJ_mark',\n",
       "  'she_PRON_nsubj',\n",
       "  'thinks_VERB_advcl',\n",
       "  'about_ADP_prep',\n",
       "  'him_PRON_pobj',\n",
       "  '._PUNCT_punct'],\n",
       " ['S The silence is surrounding her and she can help trembling while thinks about him.',\n",
       "  'A 4 5|||R:OTHER|||her,|||REQUIRED|||-NONE-|||0',\n",
       "  \"A 7 8|||R:VERB:TENSE|||can't|||REQUIRED|||-NONE-|||0\",\n",
       "  'A 11 11|||M:PRON|||she|||REQUIRED|||-NONE-|||0']]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_l_object[55]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = m_l_object\n",
    "\n",
    "with open('20200129_l8_eng_sp_error_output.json', 'w') as outfile:\n",
    "    json.dump(data, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '20200129_l8_eng_sp_error_output.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-27b9414deffb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'20200129_l8_eng_sp_error_output.txt'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mjson_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mm_l_object\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '20200129_l8_eng_sp_error_output.txt'"
     ]
    }
   ],
   "source": [
    "with open('20200129_l8_eng_sp_error_output.txt') as json_file:\n",
    "    m_l_object = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#target pronoun list\n",
    "pronouns = '(I|me|we|us|he|him|she|her|it|they|them)'\n",
    "#preps = '(on|in|for|with)'\n",
    "pro_types = '(M:PRON|R:PRON|U:PRON)'\n",
    "#prep_types = '(M:PREP|R:PREP|U:PREP)'\n",
    "pronoun_type_counter = defaultdict(lambda: defaultdict(int))\n",
    "pronoun_counter = defaultdict(int)\n",
    "\n",
    "\n",
    "for item in m_l_object:\n",
    "    #learner sentence\n",
    "    l_t = str(item[0])\n",
    "    #continue if not right part of corpus (== will yield corrected, >0 uncorrected)\n",
    "    #if annot_sents[l_t] == 0:\n",
    "            #continue\n",
    "    #learner sentence dependencies\n",
    "    l_d = item[1]\n",
    "    #corrector sentence\n",
    "    c_t = str(item[2])\n",
    "    #corrector sentence dependencies\n",
    "    c_d = item[3]\n",
    "    \n",
    "    for e in item[4]:\n",
    "        pro_target = re.search(r'\\|\\|\\|' + pro_types + r'\\|\\|\\|' \n",
    "                               + r'.*\\b(?i)' + pronouns + r'\\b.*\\|\\|\\|', e)\n",
    "        #print(l_t)\n",
    "        if pro_target:\n",
    "            anno_type = pro_target.group(1)\n",
    "            #print(anno_type)\n",
    "            current_pronoun = pro_target.group(2).lower()\n",
    "            #print(l_t, current_pronoun, e)\n",
    "            #pronoun_type_counter[anno_type] += 1\n",
    "            pronoun_type_counter[anno_type][current_pronoun] += 1\n",
    "            #pronoun_counter[current_pronoun] += 1\n",
    "            #if anno_type == 'U:PRON' and current_pronoun == 'it':\n",
    "                #print(e)\n",
    "          \n",
    "#for p in pronoun_counter.keys():\n",
    "    #print(p, pronoun_counter[p])\n",
    "for a in pronoun_type_counter.keys():\n",
    "    total = 0\n",
    "    for p in pronoun_type_counter[a].keys():\n",
    "        total += pronoun_type_counter[a][p]\n",
    "        print(a,p,pronoun_type_counter[a][p])\n",
    "    print(a, 'total: ', total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pronoun_type_counter['M:PRON'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pn_l8 = pd.DataFrame(m_l_object)\n",
    "pn_l8.head()\n",
    "#pn_l8.to_csv(r'20191118_l8_sp_eng.csv')\n",
    "#over learner corpus, M:PRON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#how many learner sentences were corrected\n",
    "\n",
    "#total number of learner sentences\n",
    "l_sents = defaultdict(int)\n",
    "#annotated sentences\n",
    "annot_sents = defaultdict(int)\n",
    "#total_annotations\n",
    "total_annots = 0\n",
    "\n",
    "for i in range(0, len(m_l_object)):\n",
    "    s = m_l_object[i][0]\n",
    "    l_sents[s]+=1\n",
    "    if i == 6623 or i == 9415 or i == 24820:\n",
    "        continue\n",
    "    no_op_search = re.search(r'\\|\\|\\|(noop)\\|\\|\\|', m_l_object[i][4][1])\n",
    "    if not no_op_search:\n",
    "        annot_sents[s]+=1\n",
    "        total_annots += 1\n",
    "    \n",
    "#Unique Learner sentences\n",
    "print(len(l_sents.keys()))\n",
    "#Learner sentences with annotations\n",
    "print(len(annot_sents.keys()))\n",
    "#Total annotations\n",
    "print(total_annots)\n",
    "\n",
    "print(10850/25974 *100)\n",
    "\n",
    "print(12883/10850)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pronoun_counter = defaultdict(int)\n",
    "\n",
    "#target pronoun list\n",
    "pronouns = '(I|me|we|us|he|him|she|her|it|they|them)'\n",
    "\n",
    "pro_target = re.search(r'\\|\\|\\|(M:PRON)\\|\\|\\|.*' + '\\b(?i)' + pronouns + '\\b.*\\|\\|\\|', l_t)\n",
    "\n",
    "\n",
    "#Learner pronoun usage over all sentences\n",
    "pro_counter_l = 0\n",
    "pro_counter_l_i = 0\n",
    "pro_counter_l_me = 0\n",
    "pro_counter_l_you = 0\n",
    "pro_counter_l_it = 0\n",
    "pro_counter_l_it_nsubj = 0\n",
    "pro_counter_l_it_nsubjpass = 0\n",
    "pro_counter_l_it_dobj = 0\n",
    "pro_counter_l_he = 0\n",
    "pro_counter_l_she = 0\n",
    "pro_counter_l_him = 0\n",
    "pro_counter_l_her = 0\n",
    "pro_counter_l_we = 0\n",
    "pro_counter_l_they = 0\n",
    "pro_counter_l_them = 0\n",
    "pro_counter_l_us = 0\n",
    "\n",
    "#Pronoun counts in Corrector Sentences\n",
    "pro_counter_c_m = 0\n",
    "\n",
    "pro_counter_c = 0\n",
    "pro_counter_c_i = 0\n",
    "pro_counter_c_me = 0\n",
    "pro_counter_c_you = 0\n",
    "pro_counter_c_it = 0\n",
    "pro_counter_c_it_nsubj = 0\n",
    "pro_counter_c_it_nsubjpass = 0\n",
    "pro_counter_c_it_dobj = 0\n",
    "pro_counter_c_he = 0\n",
    "pro_counter_c_she = 0\n",
    "pro_counter_c_him = 0\n",
    "pro_counter_c_her = 0\n",
    "pro_counter_c_we = 0\n",
    "pro_counter_c_they = 0\n",
    "pro_counter_c_them = 0\n",
    "pro_counter_c_us = 0\n",
    "\n",
    "#stats on insertions\n",
    "#all pronoun insertions\n",
    "m_pron_counter = 0\n",
    "\n",
    "i_insert = 0\n",
    "i_insert_nsubj = 0\n",
    "\n",
    "me_insert = 0\n",
    "me_insert_nsubj = 0\n",
    "me_insert_dobj = 0\n",
    "\n",
    "you_insert = 0\n",
    "you_insert_nsubj = 0\n",
    "you_insert_dobj = 0\n",
    "\n",
    "it_insert = 0\n",
    "it_insert_nsubj = 0\n",
    "it_insert_dobj = 0\n",
    "\n",
    "me_insert = 0\n",
    "\n",
    "he_insert = 0\n",
    "he_insert_nsubj = 0\n",
    "he_insert_dobj = 0\n",
    "\n",
    "she_insert = 0\n",
    "she_insert_nsubj = 0\n",
    "she_insert_dobj = 0\n",
    "\n",
    "him_insert = 0\n",
    "him_insert_nsubj = 0\n",
    "him_insert_dobj = 0\n",
    "\n",
    "her_insert = 0\n",
    "her_insert_nsubj = 0\n",
    "her_insert_dobj = 0\n",
    "\n",
    "we_insert = 0\n",
    "we_insert_nsubj = 0\n",
    "we_insert_dobj = 0\n",
    "\n",
    "us_insert = 0\n",
    "us_insert_nsubj = 0\n",
    "us_insert_dobj = 0\n",
    "\n",
    "they_insert = 0\n",
    "they_insert_nsubj = 0\n",
    "they_insert_dobj = 0\n",
    "\n",
    "them_insert = 0\n",
    "them_insert_nsubj = 0\n",
    "them_insert_dobj = 0\n",
    "\n",
    "for item in m_l_object:\n",
    "    #learner sentence\n",
    "    l_t = str(item[0])\n",
    "    #continue if not right part of corpus (== will yield corrected, >0 uncorrected)\n",
    "    if annot_sents[l_t] == 0:\n",
    "            continue\n",
    "    #learner sentence dependencies\n",
    "    l_d = item[1]\n",
    "    #corrector sentence\n",
    "    c_t = str(item[2])\n",
    "    #corrector sentence dependencies\n",
    "    c_d = item[3]\n",
    "    \n",
    "    for e in item[4]:\n",
    "            it_search = re.search(r'^A (\\d+) (\\d+)\\|\\|\\|([^\\|]+)\\|\\|\\|([^\\|]*)\\|\\|\\|', e)\n",
    "            if it_search:\n",
    "                #beginning position of token offset\n",
    "                #b_pos = int(it_search.group(1))\n",
    "                #ending position of token offset\n",
    "                #e_pos = int(it_search.group(2))\n",
    "                #annotation type ERRANT produced(M,R,U)\n",
    "                ann_type = it_search.group(3)\n",
    "                #word(s) that underwent action\n",
    "                target = str(it_search.group(4))\n",
    "                if ann_type == 'M:PRON':\n",
    "                    #550 Insertions\n",
    "                    pro_counter_c_m +=1\n",
    "                    \n",
    "                    if target == 'I':\n",
    "                        i_insert += 1\n",
    "                        for c in c_d:\n",
    "                            pro_search = re.search(target + r'\\_(PRON)\\_' r'(.*)', c)\n",
    "                            if pro_search:\n",
    "                                #print(pro_search.group())\n",
    "                                dep_list = c.split('_')\n",
    "                                #print(dep_list)\n",
    "                        if dep_list[2] == 'nsubj':\n",
    "                            i_insert_nsubj += 1\n",
    "                            \n",
    "                    elif target == 'me':\n",
    "                        me_insert += 1\n",
    "                        for c in c_d:\n",
    "                            pro_search = re.search(target + r'\\_(PRON)\\_' r'(.*)', c)\n",
    "                            if pro_search:\n",
    "                                #print(pro_search.group())\n",
    "                                dep_list = c.split('_')\n",
    "                                #print(dep_list)\n",
    "                        if dep_list[2] == 'nsubj':\n",
    "                            me_insert_nsubj += 1\n",
    "                        elif dep_list[2] == 'dobj':\n",
    "                            me_insert_dobj +=1\n",
    "                            \n",
    "                    elif target == 'you':\n",
    "                        you_insert += 1\n",
    "                        for c in c_d:\n",
    "                            pro_search = re.search(target + r'\\_(PRON)\\_' r'(.*)', c)\n",
    "                            if pro_search:\n",
    "                                #print(pro_search.group())\n",
    "                                dep_list = c.split('_')\n",
    "                                #print(dep_list)\n",
    "                        if dep_list[2] == 'nsubj':\n",
    "                            you_insert_nsubj +=1\n",
    "                        elif dep_list[2] == 'dobj':\n",
    "                            you_insert_dobj +=1\n",
    "                            \n",
    "                    elif target == 'it':\n",
    "                        it_insert += 1\n",
    "                        for c in c_d:\n",
    "                            pro_search = re.search(target + r'\\_(PRON)\\_' r'(.*)', c)\n",
    "                            if pro_search:\n",
    "                                #print(pro_search.group())\n",
    "                                dep_list = c.split('_')\n",
    "                                #print(dep_list)\n",
    "                        if dep_list[2] == 'nsubj':\n",
    "                            it_insert_nsubj +=1\n",
    "                        elif dep_list[2] == 'dobj':\n",
    "                            it_insert_dobj +=1\n",
    "                                \n",
    "                    elif target == 'It':\n",
    "                        it_insert += 1\n",
    "                        for c in c_d:\n",
    "                            pro_search = re.search(target + r'\\_(PRON)\\_' r'(.*)', c)\n",
    "                            if pro_search:\n",
    "                                #print(pro_search.group())\n",
    "                                dep_list = c.split('_')\n",
    "                                #print(dep_list)\n",
    "                        if dep_list[2] == 'nsubj':\n",
    "                            it_insert_nsubj +=1\n",
    "                        elif dep_list[2] == 'dobj':\n",
    "                            it_insert_dobj +=1\n",
    "                            \n",
    "                    elif target == 'IT':\n",
    "                        it_insert += 1\n",
    "                        for c in c_d:\n",
    "                            pro_search = re.search(target + r'\\_(PRON)\\_' r'(.*)', c)\n",
    "                            if pro_search:\n",
    "                                #print(pro_search.group())\n",
    "                                dep_list = c.split('_')\n",
    "                                #print(dep_list)\n",
    "                        if dep_list[2] == 'nsubj':\n",
    "                            it_insert_nsubj +=1\n",
    "                        elif dep_list[2] == 'dobj':\n",
    "                            it_insert_dobj +=1\n",
    "                            \n",
    "                    elif target == 'he':\n",
    "                        he_insert += 1\n",
    "                        for c in c_d:\n",
    "                            pro_search = re.search(target + r'\\_(PRON)\\_' r'(.*)', c)\n",
    "                            if pro_search:\n",
    "                                #print(pro_search.group())\n",
    "                                dep_list = c.split('_')\n",
    "                                #print(dep_list)\n",
    "                        if dep_list[2] == 'nsubj':\n",
    "                            he_insert_nsubj +=1\n",
    "                        elif dep_list[2] == 'dobj':\n",
    "                            he_insert_dobj +=1\n",
    "                            \n",
    "                    elif target == 'she':\n",
    "                        she_insert += 1\n",
    "                        for c in c_d:\n",
    "                            pro_search = re.search(target + r'\\_(PRON)\\_' r'(.*)', c)\n",
    "                            if pro_search:\n",
    "                                #print(pro_search.group())\n",
    "                                dep_list = c.split('_')\n",
    "                                #print(dep_list)\n",
    "                        if dep_list[2] == 'nsubj':\n",
    "                            she_insert_nsubj +=1\n",
    "                        elif dep_list[2] == 'dobj':\n",
    "                            she_insert_dobj +=1\n",
    "                            \n",
    "                    elif target == 'him':\n",
    "                        him_insert += 1\n",
    "                        for c in c_d:\n",
    "                            pro_search = re.search(target + r'\\_(PRON)\\_' r'(.*)', c)\n",
    "                            if pro_search:\n",
    "                                #print(pro_search.group())\n",
    "                                dep_list = c.split('_')\n",
    "                                #print(dep_list)\n",
    "                        if dep_list[2] == 'nsubj':\n",
    "                            him_insert_nsubj +=1\n",
    "                        elif dep_list[2] == 'dobj':\n",
    "                            him_insert_dobj +=1\n",
    "                            \n",
    "                    elif target == 'her':\n",
    "                        her_insert += 1\n",
    "                        for c in c_d:\n",
    "                            pro_search = re.search(target + r'\\_(PRON)\\_' r'(.*)', c)\n",
    "                            if pro_search:\n",
    "                                #print(pro_search.group())\n",
    "                                dep_list = c.split('_')\n",
    "                                #print(dep_list)\n",
    "                        if dep_list[2] == 'nsubj':\n",
    "                            her_insert_nsubj +=1\n",
    "                        elif dep_list[2] == 'dobj':\n",
    "                            her_insert_dobj +=1\n",
    "                            \n",
    "                    elif target == 'we':\n",
    "                        we_insert += 1\n",
    "                        for c in c_d:\n",
    "                            pro_search = re.search(target + r'\\_(PRON)\\_' r'(.*)', c)\n",
    "                            if pro_search:\n",
    "                                #print(pro_search.group())\n",
    "                                dep_list = c.split('_')\n",
    "                                #print(dep_list)\n",
    "                        if dep_list[2] == 'nsubj':\n",
    "                            we_insert_nsubj +=1\n",
    "                        elif dep_list[2] == 'dobj':\n",
    "                            we_insert_dobj +=1\n",
    "                        \n",
    "                    elif target == 'us':\n",
    "                        us_insert += 1\n",
    "                        for c in c_d:\n",
    "                            pro_search = re.search(target + r'\\_(PRON)\\_' r'(.*)', c)\n",
    "                            if pro_search:\n",
    "                                #print(pro_search.group())\n",
    "                                dep_list = c.split('_')\n",
    "                                #print(dep_list)\n",
    "                        if dep_list[2] == 'nsubj':\n",
    "                            us_insert_nsubj +=1\n",
    "                        elif dep_list[2] == 'dobj':\n",
    "                            us_insert_dobj +=1\n",
    "                    \n",
    "                    elif target == 'they':\n",
    "                        they_insert += 1\n",
    "                        for c in c_d:\n",
    "                            pro_search = re.search(target + r'\\_(PRON)\\_' r'(.*)', c)\n",
    "                            if pro_search:\n",
    "                                #print(pro_search.group())\n",
    "                                dep_list = c.split('_')\n",
    "                                #print(dep_list)\n",
    "                        if dep_list[2] == 'nsubj':\n",
    "                            they_insert_nsubj +=1\n",
    "                        elif dep_list[2] == 'dobj':\n",
    "                            they_insert_dobj +=1\n",
    "                            \n",
    "                    elif target == 'them':\n",
    "                        them_insert += 1\n",
    "                        for c in c_d:\n",
    "                            pro_search = re.search(target + r'\\_(PRON)\\_' r'(.*)', c)\n",
    "                            if pro_search:\n",
    "                                #print(pro_search.group())\n",
    "                                dep_list = c.split('_')\n",
    "                                #print(dep_list)\n",
    "                        if dep_list[2] == 'nsubj':\n",
    "                            them_insert_nsubj +=1\n",
    "                        elif dep_list[2] == 'dobj':\n",
    "                            them_insert_dobj +=1\n",
    "            \n",
    "            \n",
    "    for l in l_d:\n",
    "        #catpuring all pronouns in learner sentences\n",
    "        l_search = re.findall(r'\\_(PRON)\\_', l)\n",
    "        if l_search:\n",
    "            l_count = len(l_search)\n",
    "            pro_counter_l += l_count\n",
    "        #capturing all pronoun usage 'I' in learner sentences\n",
    "        l_i_search = re.findall(r'^(?i)(I)\\_', l)\n",
    "        if len(l_i_search) != 0:\n",
    "            l_i_count = len(l_i_search)\n",
    "            pro_counter_l_i += l_i_count\n",
    "        #capturing all pronoun usage 'me' in learner sentences\n",
    "        l_me_search = re.findall(r'^(?i)(Me)\\_', l)\n",
    "        if len(l_me_search) != 0:\n",
    "            l_me_count = len(l_me_search)\n",
    "            pro_counter_l_me += l_me_count\n",
    "        #capturing all pronoun usage 'you' in learner sentences\n",
    "        l_you_search = re.findall(r'^(?i)(You)\\_', l)\n",
    "        if len(l_you_search) != 0:\n",
    "            l_you_count = len(l_you_search)\n",
    "            pro_counter_l_you += l_you_count\n",
    "        #capturing all pronoun usage 'It' in learner sentences\n",
    "        l_it_search = re.findall(r'^(?i)(It)\\_(PRON)', l)\n",
    "        if len(l_it_search) != 0:\n",
    "            l_it_count = len(l_it_search)\n",
    "            pro_counter_l_it += l_it_count\n",
    "        #capturing all pronoun usage 'It' in learner sentences in 'nsubj' position\n",
    "        l_it_nsubj_search = re.findall(r'^(?i)(It)\\_(PRON)\\_(nsubj)', l)\n",
    "        if l_it_nsubj_search:\n",
    "            l_it_nsubj_search_count = len(l_it_nsubj_search)\n",
    "            pro_counter_l_it_nsubj += l_it_nsubj_search_count\n",
    "        #capturing all pronoun usage 'It' in learner sentences in 'nsubjpass' position\n",
    "        l_it_nsubjpass_search = re.findall(r'^(?i)(It)\\_(PRON)\\_(nsubjpass)', l)\n",
    "        if l_it_nsubjpass_search:\n",
    "            l_it_nsubjpass_search_count = len(l_it_nsubjpass_search)\n",
    "            pro_counter_l_it_nsubjpass += l_it_nsubjpass_search_count\n",
    "        #capturing all pronoun usage 'It' in learner sentences in 'dobj' position\n",
    "        l_it_dobj_search = re.findall(r'^(?i)(It)\\_(PRON)\\_(dobj)', l)\n",
    "        if l_it_dobj_search:\n",
    "            l_it_dobj_search_count = len(l_it_dobj_search)\n",
    "            pro_counter_l_it_dobj += l_it_dobj_search_count\n",
    "        #capturing all pronoun usage 'he' in learner sentences\n",
    "        l_he_search = re.findall(r'^(?i)(He)\\_(PRON)\\_(.*)', l)\n",
    "        if l_he_search:\n",
    "            l_he_search_count = len(l_he_search)\n",
    "            pro_counter_l_he += l_he_search_count\n",
    "        #capturing all pronoun usage 'She' in learner sentences\n",
    "        l_she_search = re.findall(r'^(?i)(She)\\_(PRON)\\_(.*)', l)\n",
    "        if l_she_search:\n",
    "            l_she_search_count = len(l_she_search)\n",
    "            pro_counter_l_she += l_she_search_count\n",
    "        #capturing all pronoun usage 'him' in learner sentences\n",
    "        l_him_search = re.findall(r'^(?i)(Him)\\_(PRON)\\_(.*)', l)\n",
    "        if l_him_search:\n",
    "            l_him_search_count = len(l_him_search)\n",
    "            pro_counter_l_him += l_him_search_count\n",
    "        #capturing all pronoun usage 'her' in learner sentences\n",
    "        l_her_search = re.findall(r'^(?i)(Her)\\_(PRON)\\_(.*)', l)\n",
    "        if l_her_search:\n",
    "            l_her_search_count = len(l_her_search)\n",
    "            pro_counter_l_her += l_her_search_count\n",
    "        #capturing all pronoun usage 'we' in learner sentences\n",
    "        l_we_search = re.findall(r'^(?i)(We)\\_(PRON)\\_(.*)', l)\n",
    "        if l_we_search:\n",
    "            l_we_search_count = len(l_we_search)\n",
    "            pro_counter_l_we += l_we_search_count\n",
    "        #capturing all pronoun usage 'us' in learner sentences\n",
    "        l_us_search = re.findall(r'^(?i)(Us)\\_(PRON)\\_(.*)', l)\n",
    "        if l_us_search:\n",
    "            l_us_search_count = len(l_us_search)\n",
    "            pro_counter_l_us += l_us_search_count\n",
    "        #capturing all pronoun usage 'they' in learner sentences\n",
    "        l_they_search = re.findall(r'^(?i)(They)\\_(PRON)\\_(.*)', l)\n",
    "        if l_they_search:\n",
    "            l_they_search_count = len(l_they_search)\n",
    "            pro_counter_l_they += l_they_search_count\n",
    "        #capturing all pronoun usage 'them' in learner sentences\n",
    "        l_them_search = re.findall(r'^(?i)(Them)\\_(PRON)\\_(.*)', l)\n",
    "        if l_them_search:\n",
    "            l_them_search_count = len(l_them_search)\n",
    "            pro_counter_l_them += l_them_search_count\n",
    "            \n",
    "            \n",
    "    for c in c_d:\n",
    "        #capturing all pronouns in corrector sentences\n",
    "        c_search = re.findall(r'\\_(PRON)\\_', c)\n",
    "        if c_search:\n",
    "            c_count = len(c_search)\n",
    "            pro_counter_c += c_count\n",
    "        #capturing pronoun usage 'I' in corrector sentences\n",
    "        c_i_search = re.findall(r'^(?i)(I)\\_', c)\n",
    "        if len(c_i_search) != 0:\n",
    "            c_i_count = len(c_i_search)\n",
    "            pro_counter_c_i += c_i_count\n",
    "        #capturing all pronoun usage 'me' in corrector sentences\n",
    "        c_me_search = re.findall(r'^(?i)(Me)\\_', c)\n",
    "        if len(c_me_search) != 0:\n",
    "            c_me_count = len(c_me_search)\n",
    "            pro_counter_c_me += c_me_count\n",
    "        #capturing all pronoun usage 'you' in learner sentences\n",
    "        c_you_search = re.findall(r'^(?i)(You)\\_', c)\n",
    "        if len(c_you_search) != 0:\n",
    "            c_you_count = len(c_you_search)\n",
    "            pro_counter_c_you += c_you_count\n",
    "        #capturing pronoun usage 'It' in corrector sentences\n",
    "        c_it_search = re.findall(r'^(?i)(It)\\_', c)\n",
    "        if len(c_it_search) != 0:\n",
    "            c_it_count = len(c_it_search)\n",
    "            pro_counter_c_it += c_it_count\n",
    "         #capturing all pronoun usage 'It' in corrector sentences in 'nsubj' position\n",
    "        c_it_nsubj_search = re.findall(r'^(?i)(It)\\_(PRON)\\_(nsubj)', c)\n",
    "        if c_it_nsubj_search:\n",
    "            c_it_nsubj_search_count = len(c_it_nsubj_search)\n",
    "            pro_counter_c_it_nsubj += c_it_nsubj_search_count\n",
    "        #capturing all pronoun usage 'It' in corrector sentences in 'nsubjpass' position\n",
    "        c_it_nsubjpass_search = re.findall(r'^(?i)(It)\\_(PRON)\\_(nsubjpass)', c)\n",
    "        if c_it_nsubjpass_search:\n",
    "            c_it_nsubjpass_search_count = len(c_it_nsubjpass_search)\n",
    "            pro_counter_c_it_nsubjpass += c_it_nsubjpass_search_count\n",
    "        #capturing all pronoun usage 'It' in corrector sentences in 'dobj' position\n",
    "        c_it_dobj_search = re.findall(r'^(?i)(It)\\_(PRON)\\_(dobj)', c)\n",
    "        if c_it_dobj_search:\n",
    "            c_it_dobj_search_count = len(c_it_dobj_search)\n",
    "            pro_counter_c_it_dobj += c_it_dobj_search_count\n",
    "        #capturing all pronoun usage 'he' in corrector sentences\n",
    "        c_he_search = re.findall(r'^(?i)(He)\\_(PRON)\\_(.*)', c)\n",
    "        if c_he_search:\n",
    "            c_he_search_count = len(c_he_search)\n",
    "            pro_counter_c_he += c_he_search_count\n",
    "        #capturing all pronoun usage 'She' in corrector sentences\n",
    "        c_she_search = re.findall(r'^(?i)(She)\\_(PRON)\\_(.*)', c)\n",
    "        if c_she_search:\n",
    "            c_she_search_count = len(c_she_search)\n",
    "            pro_counter_c_she += c_she_search_count\n",
    "        #capturing all pronoun usage 'him' in corrector sentences\n",
    "        c_him_search = re.findall(r'^(?i)(Him)\\_(PRON)\\_(.*)', c)\n",
    "        if c_him_search:\n",
    "            c_him_search_count = len(c_him_search)\n",
    "            pro_counter_c_him += c_him_search_count\n",
    "        #capturing all pronoun usage 'her' in corrector sentences\n",
    "        c_her_search = re.findall(r'^(?i)(Her)\\_(PRON)\\_(.*)', c)\n",
    "        if c_her_search:\n",
    "            c_her_search_count = len(c_her_search)\n",
    "            pro_counter_c_her += c_her_search_count\n",
    "        #capturing all pronoun usage 'we' in corrector sentences\n",
    "        c_we_search = re.findall(r'^(?i)(We)\\_(PRON)\\_(.*)', c)\n",
    "        if c_we_search:\n",
    "            c_we_search_count = len(c_we_search)\n",
    "            pro_counter_c_we += c_we_search_count\n",
    "        #capturing all pronoun usage 'us' in corrector sentences\n",
    "        c_us_search = re.findall(r'^(?i)(Us)\\_(PRON)\\_(.*)', c)\n",
    "        if c_us_search:\n",
    "            c_us_search_count = len(c_us_search)\n",
    "            pro_counter_c_us += c_us_search_count\n",
    "        #capturing all pronoun usage 'they' in learner sentences\n",
    "        c_they_search = re.findall(r'^(?i)(They)\\_(PRON)\\_(.*)', c)\n",
    "        if c_they_search:\n",
    "            c_they_search_count = len(c_they_search)\n",
    "            pro_counter_c_they += c_they_search_count\n",
    "        #capturing all pronoun usage 'them' in learner sentences\n",
    "        c_them_search = re.findall(r'^(?i)(Them)\\_(PRON)\\_(.*)', c)\n",
    "        if c_them_search:\n",
    "            c_them_search_count = len(c_them_search)\n",
    "            pro_counter_c_them += c_them_search_count\n",
    "    \n",
    "\n",
    "#general stats\n",
    "#print(m_pron_counter)\n",
    "print(\"Instances of Pronoun usage in learner sentences: \", pro_counter_l)\n",
    "print(\"Instances of Pronoun usage in corrector sentences: \", pro_counter_c)\n",
    "print('Number of corrections in total: ', num, num/len(l_c_dict.keys())*100)\n",
    "print(\"Instances of Pronouns 'inserted' by corrector: \", pro_counter_c_m)\n",
    "print(\"Learner pronoun accuracy over corrections: \", pro_counter_l/pro_counter_l+pro_counter_c)\n",
    "print(\"Pronoun  usage 'I' in learner sentences: \", pro_counter_l_i)\n",
    "print(\"Pronoun usage 'I' in corrector sentences: \", pro_counter_c_i)\n",
    "print(\"Number of times 'I' was inserted by corrector: \", i_insert)\n",
    "print(\"Number of times 'I' was inserted at 'nsubj' position: \", i_insert_nsubj)\n",
    "print(\"Pronoun usage 'me' in learner sentences: \", pro_counter_l_me)\n",
    "print(\"Pronoun usage 'me' in corrector sentences: \", pro_counter_c_me)\n",
    "print(\"Number of times 'me' was inserted by corrector: \", me_insert)\n",
    "print(\"Number of times 'me' was inserted by corrector (nsubj): \", me_insert_nsubj)\n",
    "print(\"Number of times 'me' was inserted by corrector (dobj): \", me_insert_dobj)\n",
    "print(\"Pronoun usage 'you' in learner sentences: \", pro_counter_l_you)\n",
    "print(\"Pronoun usage 'you' in corrector sentences: \", pro_counter_c_you)\n",
    "print(\"Number of times 'you' was inserted by corrector: \", you_insert)\n",
    "print(\"Number of times 'you' was inserted by corrector (nsubj): \", you_insert_nsubj)\n",
    "print(\"Number of times 'you' was inserted by corrector (dobj): \", you_insert_dobj)\n",
    "print(\"Pronoun usage 'It' in learner sentences: \", pro_counter_l_it)\n",
    "print(\"Pronoun usage 'It' in learner sentences (nsubj position): \", pro_counter_l_it_nsubj)\n",
    "print(\"Pronoun usage 'It' in learner sentences (dobj position): \", pro_counter_l_it_dobj)\n",
    "print(\"Pronoun usage 'It' in corrector sentences: \", pro_counter_c_it)\n",
    "print(\"Pronoun usage 'It' in corrector sentences (nsubj position): \", pro_counter_c_it_nsubj)\n",
    "print(\"Pronoun usage 'It' in corrector sentences (dobj position): \", pro_counter_c_it_dobj)\n",
    "print(\"Number of times 'It' was inserted by corrector: \", it_insert)\n",
    "print(\"Number of times 'It' was inserted by corrector (nsubj): \", it_insert_nsubj)\n",
    "print(\"Number of times 'It' was inserted by corrector (dobj): \", it_insert_dobj)\n",
    "print(\"Pronoun usage 'he' in learner setence: \", pro_counter_l_he)\n",
    "print(\"Pronoun usage 'he' in corrector sentence: \", pro_counter_c_he)\n",
    "print(\"Number of times 'he' was inserted by corrector: \", he_insert)\n",
    "print(\"Number of times 'he' was inserted by corrector (nsubj): \", he_insert_nsubj)\n",
    "print(\"Number of times 'he' was inserted by corrector (dobj): \", he_insert_dobj)\n",
    "print(\"Pronoun usage 'she' in learner sentence: \", pro_counter_l_she)\n",
    "print(\"Pronoun usage 'she' in corrector sentence: \", pro_counter_c_she)\n",
    "print(\"Number of times 'she' was inserted by corrector : \", she_insert)\n",
    "print(\"Number of times 'she' was inserted by corrector (nsubj): \", she_insert_nsubj)\n",
    "print(\"Number of times 'she' was inserted by corrector (dobj): \", she_insert_dobj)\n",
    "print(\"Pronoun usage 'him' in learner sentence: \", pro_counter_l_him)\n",
    "print(\"Pronoun usage 'him' in corrector sentence: \", pro_counter_c_him)\n",
    "print(\"Number of times 'him' was inserted by corrector : \", him_insert)\n",
    "print(\"Number of times 'him' was inserted by corrector (nsubj) : \", him_insert_nsubj)\n",
    "print(\"Number of times 'him' was inserted by corrector (dobj) : \", him_insert_dobj)\n",
    "print(\"Pronoun usage 'her' in learner sentence: \", pro_counter_l_her)\n",
    "print(\"Pronoun usage 'her' in corrector sentence: \", pro_counter_c_her)\n",
    "print(\"Number of times 'her' was inserted by corrector: \", her_insert)\n",
    "print(\"Number of times 'her' was inserted by corrector (nsubj) : \", her_insert_nsubj)\n",
    "print(\"Number of times 'her' was inserted by corrector (dobj) : \", her_insert_dobj)\n",
    "print(\"Pronoun usage 'we' in learner sentence: \", pro_counter_l_we)\n",
    "print(\"Pronoun usage 'we' in corrector sentence: \", pro_counter_c_we)\n",
    "print(\"Number of times 'we' was inserted by corrector: \", we_insert)\n",
    "print(\"Pronoun usage 'us' in learner sentence: \", pro_counter_l_us)\n",
    "print(\"Pronoun usage 'us' in corrector sentence: \", pro_counter_c_us)\n",
    "print(\"Number of times 'us' was inserted by corrector (nsubj) : \", us_insert_nsubj)\n",
    "print(\"Number of times 'us' was inserted by corrector (dobj) : \", us_insert_dobj)\n",
    "print(\"Pronoun usage 'they' in learner sentence: \", pro_counter_l_they)\n",
    "print(\"Pronoun usage 'they' in corrector sentence: \", pro_counter_c_they)\n",
    "print(\"Number of times 'they' was inserted by corrector: \", they_insert)\n",
    "print(\"Number of times 'they' was inserted by corrector (nsubj): \", they_insert_nsubj)\n",
    "print(\"Number of times 'they' was inserted by corrector (dobj): \", they_insert_dobj)\n",
    "print(\"Pronoun usage 'them' in learner sentence: \", pro_counter_l_them)\n",
    "print(\"Pronoun usage 'them' in corrector sentence: \", pro_counter_c_them)\n",
    "print(\"Number of times 'them' was inserted by corrector: \", them_insert)\n",
    "print(\"Number of times 'them' was inserted by corrector (nsubj): \", them_insert_nsubj)\n",
    "print(\"Number of times 'them' was inserted by corrector (dobj): \", them_insert_dobj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in m_l_object:\n",
    "    #learner sentence\n",
    "    l_t = nlp(item[0])\n",
    "    l_t = [i.text for i in l_t]\n",
    "    #continue if not right part of corpus (== will yield corrected, >0 uncorrected)\n",
    "    #if annot_sents[l_t] == 0:\n",
    "            #continue\n",
    "    #learner sentence dependencies\n",
    "    l_d = item[1]\n",
    "    #corrector sentence\n",
    "    c_t = str(item[2])\n",
    "    #corrector sentence dependencies\n",
    "    c_d = item[3]\n",
    "    \n",
    "    for e in item[4]:\n",
    "            run_search = re.search(r'^A (\\d+) (\\d+)\\|\\|\\|([^\\|]+)\\|\\|\\|([^\\|]*)\\|\\|\\|', e)\n",
    "            if run_search:\n",
    "                #beginning position of token offset\n",
    "                b_pos = int(run_search.group(1))\n",
    "                #ending position of token offset\n",
    "                e_pos = int(run_search.group(2))\n",
    "                #annotation type ERRANT produced(M,R,U)\n",
    "                ann_type = run_search.group(3)\n",
    "                #word(s) that underwent action\n",
    "                target = str(run_search.group(4))\n",
    "                if target == '.':\n",
    "                    print(l_t[b_pos])\n",
    "                        #if item == ',':\n",
    "                            #print(item)\n",
    "                            #print(l_t,c_t,ann_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Realignment\n",
    "#following dependencies will add to an adjustment\n",
    "\n",
    "with open('20191108_error_output','w') as output:\n",
    "    for item in m_l_object:\n",
    "            #learner text\n",
    "            l_t = str(item[0])\n",
    "            #corrector sentence\n",
    "            c_t = str(item[2])\n",
    "            #corrector sentence dependencies\n",
    "            c_dp = item[3]\n",
    "            #annotation adjustments based on possible insertions\n",
    "            ann_adjust = 0\n",
    "            for e in item[4]:\n",
    "                #try:\n",
    "                    #re.search for ERRANT: ||| (it) ||| in annotations\n",
    "                    #what if there is more than one 'it' in annotation?\n",
    "                    it_search = re.search(r'^A (\\d+) (\\d+)\\|\\|\\|([^\\|]+)\\|\\|\\|([^\\|]*)\\|\\|\\|', e)\n",
    "                    if it_search:\n",
    "                        #beginning position of token offset\n",
    "                        b_pos = int(it_search.group(1))\n",
    "                        #ending position of token offset\n",
    "                        e_pos = int(it_search.group(2))\n",
    "                        #annotation type ERRANT produced(M,R,U)\n",
    "                        ann_type = it_search.group(3)\n",
    "                        #word(s) that underwent action\n",
    "                        target = it_search.group(4)\n",
    "                        #length of error token(s)\n",
    "                        err_length = e_pos - b_pos\n",
    "                        #length of token(s) in question\n",
    "                        t_length = len(target.split())\n",
    "                        t_list = target.split()\n",
    "                        #if dependency matches any tags in list, adjust by +1\n",
    "                        t_len = 0\n",
    "                        l_list = l_t.split()\n",
    "                        l_list = l_list[:b_pos]\n",
    "                        l_list = ' '.join([i for i in l_list])\n",
    "                        l_added_tokens = re.findall(r'[\\'\\,\\.\\?\\!\\-\\\"\\;\\:\\@\\’\\`\\“]', l_list)\n",
    "                        print(l_added_tokens)\n",
    "                        l_adjust = len(l_added_tokens)\n",
    "                        c_list = c_t.split()\n",
    "                        c_list = c_list[:b_pos]\n",
    "                        c_list = ' '.join([c for c in c_list])\n",
    "                        c_added_hyphen = re.findall(r'[\\-]', c_list)\n",
    "                        l_adjust += len(c_added_hyphen)\n",
    "                        for t in t_list:\n",
    "                            p_search = re.findall(r'[\\'\\,\\.\\?\\!\\-\\\"\\;\\:\\@\\’\\`\\“]', t)\n",
    "                            t_len = len(p_search)\n",
    "                        print(ann_type, l_t, c_t, e, b_pos, e_pos, target, \n",
    "                              c_dp[b_pos+ l_adjust + ann_adjust:e_pos+ l_adjust + ann_adjust+t_length + t_len - err_length], ann_adjust, c_dp)\n",
    "                        print(b_pos+ann_adjust, e_pos+t_length - err_length, t_len, l_adjust)\n",
    "                        ann_adjust += t_length + t_len - err_length\n",
    "\n",
    "#Up to the position of the error in the string of the original sentence (findall punct on the)\n",
    "#learner sentence. For the learner sentence, do a slice up until error, then .join(), that list\n",
    "#that goes from 0 to the current errors (b-pos), doing r.findall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Teacher Judgement Table (Pandas)\n",
    "t_j_dict = {'Nature of Problem': ['Subject-Verb Agreement', 'Pro-Drop (it-2)',\n",
    "                                'Mistaken vowel usage stemming from L1 (e.g: Live vs. Leave)',\n",
    "                                'Word tense', 'Conjugation', 'Pluralization of Adjectives',\n",
    "                                'Lexical Errors(e.g: history/story, make/do for collocations)',\n",
    "                                'I am agree', 'Lack of possesive \"s\"', 'Absence of the plural marker \"s\"',\n",
    "                                'Question formation',\n",
    "                                'Mistake in gender aspect of pronouns',\n",
    "                                'Omission of \"do\" or misuse of variation (e.g: \"He do good work\")',\n",
    "                                'Prepositions (e.g: \"Listen me\" vs. \"Listen to me\")',\n",
    "                                'Interchanging of the subject, object and possessive pronouns (e.g: Usage of \"me\" vs \"my\")',\n",
    "                                'Infinitival \"to\" (e.g: \"for to\" vs \"in order to\")',\n",
    "                                'Word (direct translation)', 'Reversal of the adjective and noun' ,\n",
    "                                'Adding \"e\" to the beggining of words with cosonant clusters(sc..)',\n",
    "                                'Run-on sentences', 'Lack of period usage/other gramm. tools',\n",
    "                                'Confusion of this/that, these/those',\n",
    "                                'Addition of \"s\" to words that do not require it (e.g: thank yous)',\n",
    "                                'Absence of the word \"there\" in \"there are\" and \"there is\" from \"hay\"'\n",
    "                               ],\n",
    "         'Count':[3, 3, 1, 1,2, 1, 1, 1, 1, 1, 2, 1, 2, 1, 2, 1, 1, 1, 1, 4, \n",
    "                 1, 1, 1, 1],\n",
    "         'Category': [\n",
    "             'Grammatical', 'Grammatical', 'Grammatical/Spelling',\n",
    "             'Grammatical', 'Grammatical', 'Grammatical', 'Grammatical/Phonetic',\n",
    "             'Grammatical/Phonetic', 'Grammatical', 'Grammatical', 'Grammatical',\n",
    "             'Grammatical', 'Grammatical', 'Grammatical', 'Grammatical', 'Grammatical',\n",
    "             'Grammatical', 'Grammitcal/Word Order', 'Spelling/Phonetic',\n",
    "             'Grammar/Punctuation','Punctuation', 'Deictic',\n",
    "             'Other', 'Grammatical']\n",
    "         }\n",
    "t_j_df = pd.DataFrame(t_j_dict)\n",
    "t_j_df.sort_values(by=['Count'], inplace = True, ascending = False)\n",
    "t_j_df.reset_index(drop=True)\n",
    "#t_j_df.to_csv(r'20191202_l8_teacher_judgements.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_text = \"You are attending a CiRCL presentation\"\n",
    "t_text = nlp(t_text)\n",
    "svg = displacy.render(t_text, style='dep')\n",
    "\n",
    "#output_path = Path(\"dependency_plot.svg\") \n",
    "#output_path.open(\"w\", encoding=\"utf-8\").write(svg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_text = \"You are attending, a CiRCL presentation\"\n",
    "t_text = nlp(t_text)\n",
    "\n",
    "for t in t_text:\n",
    "    print(t.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
